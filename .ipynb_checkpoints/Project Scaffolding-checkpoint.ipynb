{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"weather_data\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones\n",
    "In this section, we loaded the taxi shapefile that corresponds location IDs to geographical latitudes and logitudes. \n",
    "* The `load_taxi_zones` function reads the shapefile and use GeoPandas to read the file\n",
    "* The `lookup_coords_for_taxi_zone_id` function takes location IDs and the loaded shapefile and returns a tuple of latitude and logitude\n",
    "* The `make_loc_id_coords_dict` creates a dictionary of location IDs and coordinates that will be used in data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    taxi_zones = gpd.read_file(shapefile)\n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5296e8e3-e831-4d68-b1a2-8af4d49c949d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Projected CRS: EPSG:2263>\n",
       "Name: NAD83 / New York Long Island (ftUS)\n",
       "Axis Info [cartesian]:\n",
       "- X[east]: Easting (US survey foot)\n",
       "- Y[north]: Northing (US survey foot)\n",
       "Area of Use:\n",
       "- name: United States (USA) - New York - counties of Bronx; Kings; Nassau; New York; Queens; Richmond; Suffolk.\n",
       "- bounds: (-74.26, 40.47, -71.8, 41.3)\n",
       "Coordinate Operation:\n",
       "- name: SPCS83 New York Long Island zone (US survey foot)\n",
       "- method: Lambert Conic Conformal (2SP)\n",
       "Datum: North American Datum 1983\n",
       "- Ellipsoid: GRS 1980\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "taxi_zones.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a924269c-1348-4110-ba72-326f2f258264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones=taxi_zones):\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones[\"LocationID\"] == zone_loc_id]\n",
    "    centroid = zone.geometry.centroid.iloc[0]\n",
    "    centroid_geo = gpd.GeoSeries([centroid], crs=loaded_taxi_zones.crs).to_crs(epsg=CRS).iloc[0]\n",
    "\n",
    "    latitude = centroid_geo.y\n",
    "    longitude = centroid_geo.x\n",
    "\n",
    "    return (latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae65882-f709-4348-80a0-6276cd04eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loc_id_coords_dict(loaded_taxi_zones):\n",
    "    id_coords_dict = {}\n",
    "    for loc_id in loaded_taxi_zones[\"LocationID\"]:\n",
    "        id_coords_dict[loc_id] = lookup_coords_for_taxi_zone_id(loc_id, loaded_taxi_zones)\n",
    "\n",
    "    return id_coords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90e63ccc-1ff3-46cf-8252-d402e9cf1a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ID_COORDS_DICT = make_loc_id_coords_dict(taxi_zones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population):\n",
    "    confidence_level = 0.95\n",
    "    margin_of_error = 0.05\n",
    "    proportion = 0.5\n",
    "    \n",
    "    from scipy.stats import norm\n",
    "\n",
    "    z_score = norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "\n",
    "    # Cochranâ€™s\n",
    "    n = (z_score**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population\n",
    "    n_adj = n / (1 + (n - 1) / population)\n",
    "    \n",
    "    return int(round(n_adj)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions\n",
    "* `get_all_urls_from_taxi_page` fetches information on the taxi page and finds all \"Yellow Taxi Trip Records\" urls and \"High Volume For-Hire Vehicle Trip Records\" urls\n",
    "* `find_parquet_urls` uses regex to filter the urls that ends with \".parquet\" to make sure that the urls are parquet files\n",
    "* `download_parquet` creates a directory and downloads relevant parquets into the directory\n",
    "* `get_and_clean_month` filters the urls that are from January 2020 to August 2024\n",
    "* `sample_monthly` function reads all the parquet files in a directory, finds the file with largest number of rows and computes the sample size using the \"maximum population\". Next it creates samples for all files using the computed sample size and combine them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    response = requests.get(taxi_page)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    yellow_tags = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    "    fhvhv_tags = soup.find_all(\"a\", attrs={\"title\": \"High Volume For-Hire Vehicle Trip Records\"})\n",
    "\n",
    "    yellow_urls = [a[\"href\"].strip() for a in yellow_tags]\n",
    "    fhvhv_urls = [a[\"href\"].strip() for a in fhvhv_tags]\n",
    "    \n",
    "    return yellow_urls, fhvhv_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parquet_urls(urls):\n",
    "    pattern = re.compile(r\"\\.parquet$\")\n",
    "    parquet_urls = [url for url in urls if pattern.search(url)]    \n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b77c3ef5-812f-45db-bed1-d9f8ef52269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parquet(urls, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for url in urls:\n",
    "        filename = os.path.basename(url)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        if os.path.exists(output_path):\n",
    "            continue        \n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded {filename} to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(urls):\n",
    "    pattern = re.compile(r\"(202[0-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\")\n",
    "    cleaned_urls = [url for url in urls if pattern.search(url)]\n",
    "    return cleaned_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2c60ef2-3ab2-44f2-938e-05cbf8868f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_to_df(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".parquet\")]\n",
    "    all_dataframe = []\n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_parquet(file)\n",
    "        all_dataframe.append(df)\n",
    "    if all_dataframe:\n",
    "        combined_df = pd.concat(all_dataframe, ignore_index=True)\n",
    "        return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "116a75c2-0bfb-4149-9548-47a35f3f2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parquet_column(file_path, columns_to_keep):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    cleaned_df = df[columns_to_keep]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7e11d08-6d62-4acd-bf15-3e1fe5ec5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_urls, fhvhv_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "taxi_parquet = find_parquet_urls(yellow_urls)\n",
    "uber_parquet = find_parquet_urls(fhvhv_urls)\n",
    "taxi_urls = get_and_clean_month(taxi_parquet)\n",
    "uber_urls = get_and_clean_month(uber_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6396fb8-bea9-4db3-a918-59eeeee34518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_monthly(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    \n",
    "    max_rows = 0\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        max_rows = max(max_rows, len(df))\n",
    "\n",
    "    sample_size = calculate_sample_size(max_rows)\n",
    "    print(f\"Sample size for all months: {sample_size}\")\n",
    "\n",
    "    sampled_dataframes = []\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        sampled_df = df.sample(n=sample_size, random_state=30, replace=False)\n",
    "        sampled_dataframes.append(sampled_df)\n",
    "    print(\"Finished sampling\")\n",
    "\n",
    "    if sampled_dataframes:\n",
    "        combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n",
    "        return combined_sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb0a333f-84b4-40c6-86e7-f2d56ecba837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded yellow_tripdata_2024-01.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2024-02.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2024-03.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2024-04.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2024-05.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2024-06.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2024-07.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2024-08.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-01.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-02.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-03.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-04.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-05.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-06.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-07.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-08.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-09.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-10.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-11.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2023-12.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-01.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-02.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-03.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-04.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-05.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-06.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-07.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-08.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-09.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-10.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-11.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2022-12.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-01.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-02.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-03.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-04.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-05.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-06.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-07.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-08.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-09.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-10.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-11.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2021-12.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-01.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-02.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-03.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-04.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-05.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-06.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-07.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-08.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-09.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-10.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-11.parquet to taxi_data\n",
      "Downloaded yellow_tripdata_2020-12.parquet to taxi_data\n"
     ]
    }
   ],
   "source": [
    "taxi_data_dir = \"taxi_data\"\n",
    "download_parquet(taxi_urls, taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "882a02c9-5fcc-45ac-bdfc-c3b6fb6f1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for all months: 384\n",
      "Finished sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NIU\\AppData\\Local\\Temp\\ipykernel_5408\\1458695063.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "sampled_taxi_df = sample_monthly(taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49122c36-026f-439e-97ff-82ea5515d31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-21 15:41:03</td>\n",
       "      <td>2020-01-21 15:54:32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>233</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>14.14</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-29 21:06:16</td>\n",
       "      <td>2020-01-29 21:11:34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>141</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-26 20:19:48</td>\n",
       "      <td>2020-01-26 20:25:39</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-16 13:17:34</td>\n",
       "      <td>2020-01-16 13:24:01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>143</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.76</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-30 20:20:32</td>\n",
       "      <td>2020-01-30 20:28:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>233</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-29 19:21:58</td>\n",
       "      <td>2020-01-29 19:35:52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>90</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.88</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-12 17:42:09</td>\n",
       "      <td>2020-01-12 17:47:26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>236</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-29 22:03:48</td>\n",
       "      <td>2020-01-29 22:08:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-21 21:17:23</td>\n",
       "      <td>2020-01-21 21:36:29</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-15 01:27:14</td>\n",
       "      <td>2020-01-15 01:32:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.70</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2020-01-21 15:41:03   2020-01-21 15:54:32              2.0   \n",
       "1         2  2020-01-29 21:06:16   2020-01-29 21:11:34              1.0   \n",
       "2         2  2020-01-26 20:19:48   2020-01-26 20:25:39              3.0   \n",
       "3         2  2020-01-16 13:17:34   2020-01-16 13:24:01              1.0   \n",
       "4         2  2020-01-30 20:20:32   2020-01-30 20:28:30              1.0   \n",
       "5         2  2020-01-29 19:21:58   2020-01-29 19:35:52              1.0   \n",
       "6         1  2020-01-12 17:42:09   2020-01-12 17:47:26              2.0   \n",
       "7         1  2020-01-29 22:03:48   2020-01-29 22:08:19              1.0   \n",
       "8         2  2020-01-21 21:17:23   2020-01-21 21:36:29              4.0   \n",
       "9         1  2020-01-15 01:27:14   2020-01-15 01:32:45              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           0.71         1.0                  N           233           164   \n",
       "1           0.76         1.0                  N           141           140   \n",
       "2           0.77         1.0                  N           161           163   \n",
       "3           0.99         1.0                  N           143           239   \n",
       "4           0.84         1.0                  N           233           162   \n",
       "5           1.54         1.0                  N            90           161   \n",
       "6           1.10         1.0                  N           140           236   \n",
       "7           0.90         1.0                  N           142           239   \n",
       "8           3.78         1.0                  N           161           148   \n",
       "9           0.90         1.0                  N           161           233   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1          9.0    0.0      0.5        1.84           0.0   \n",
       "1             2          5.5    0.5      0.5        0.00           0.0   \n",
       "2             2          5.5    0.5      0.5        0.00           0.0   \n",
       "3             1          6.5    0.0      0.5        1.96           0.0   \n",
       "4             1          6.5    0.5      0.5        1.20           0.0   \n",
       "5             1         10.0    1.0      0.5        3.58           0.0   \n",
       "6             2          6.0    2.5      0.5        0.00           0.0   \n",
       "7             2          5.5    3.0      0.5        0.00           0.0   \n",
       "8             1         15.0    0.5      0.5        3.76           0.0   \n",
       "9             1          6.0    3.0      0.5        2.90           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \\\n",
       "0                    0.3         14.14                   2.5          NaN   \n",
       "1                    0.3          9.30                   2.5          NaN   \n",
       "2                    0.3          9.30                   2.5          NaN   \n",
       "3                    0.3         11.76                   2.5          NaN   \n",
       "4                    0.3         11.50                   2.5          NaN   \n",
       "5                    0.3         17.88                   2.5          NaN   \n",
       "6                    0.3          9.30                   2.5          NaN   \n",
       "7                    0.3          9.30                   2.5          NaN   \n",
       "8                    0.3         22.56                   2.5          NaN   \n",
       "9                    0.3         12.70                   2.5          NaN   \n",
       "\n",
       "   Airport_fee  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_taxi_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "731be965-3963-443f-97cf-d57d9d826bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxidata(dataframe):\n",
    "    try: \n",
    "        print(f\"Cleaning the sample dataframe...\")\n",
    "\n",
    "        if not isinstance(dataframe, pd.DataFrame):\n",
    "            raise ValueError(\"must Pandas DataFrame\")\n",
    "\n",
    "        # look up the latitude and longitude (get those coordinates)\n",
    "        dataframe[[\"latitude_pickup\", \"longitude_pickup\"]] = dataframe[\"PULocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "        dataframe[[\"latitude_dropoff\", \"longitude_dropoff\"]] = dataframe[\"DOLocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "\n",
    "        \n",
    "        # remove some location IDs not valid and distance is 0\n",
    "        dataframe = dataframe.dropna(subset=['latitude_pickup', 'longitude_pickup', 'latitude_dropoff', 'longitude_dropoff'])\n",
    "        dataframe = dataframe[dataframe[\"trip_distance\"] != 0]\n",
    "\n",
    "\n",
    "        # remove unnecessary columns\n",
    "        columns_to_keep = [\n",
    "            'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "            'trip_distance', \n",
    "            'latitude_pickup', 'longitude_pickup', 'latitude_dropoff', 'longitude_dropoff' , \n",
    "            'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee', \n",
    "            'total_amount'\n",
    "        ]         #è°ƒæ•´\n",
    "        dataframe = dataframe[columns_to_keep]\n",
    "\n",
    "\n",
    "        # normalize column names\n",
    "        dataframe.columns = [col.lower().replace(' ', '_') for col in dataframe.columns]\n",
    "\n",
    "        # normalizing and using appropriate column types for the respective data;\n",
    "        dataframe['tpep_pickup_datetime'] = pd.to_datetime(dataframe['tpep_pickup_datetime'])\n",
    "        dataframe['tpep_dropoff_datetime'] = pd.to_datetime(dataframe['tpep_dropoff_datetime'])\n",
    "        dataframe['trip_distance'] = dataframe['trip_distance'].astype(float)\n",
    "\n",
    "        # for Yellow Taxi data, remove trips that start and/or end outside of  (40.560445, -74.242330) and (40.908524, -73.717047) ie NEW_YORK_BOX_COORDS.\n",
    "        lat_min, lon_min = NEW_YORK_BOX_COORDS[0]\n",
    "        lat_max, lon_max = NEW_YORK_BOX_COORDS[1]\n",
    "        \n",
    "        dataframe = dataframe[\n",
    "            (dataframe['latitude_pickup'].between(lat_min, lat_max)) &\n",
    "            (dataframe['longitude_pickup'].between(lon_min, lon_max)) &\n",
    "            (dataframe['latitude_dropoff'].between(lat_min, lat_max)) &\n",
    "            (dataframe['longitude_dropoff'].between(lon_min, lon_max))\n",
    "        ]\n",
    "\n",
    "       \n",
    "        \n",
    "        return dataframe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the dataframe: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "508ab469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the sample dataframe...\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxidata(sampled_taxi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>latitude_pickup</th>\n",
       "      <th>longitude_pickup</th>\n",
       "      <th>latitude_dropoff</th>\n",
       "      <th>longitude_dropoff</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21 15:41:03</td>\n",
       "      <td>2020-01-21 15:54:32</td>\n",
       "      <td>0.71</td>\n",
       "      <td>40.749914</td>\n",
       "      <td>-73.970443</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-29 21:06:16</td>\n",
       "      <td>2020-01-29 21:11:34</td>\n",
       "      <td>0.76</td>\n",
       "      <td>40.766948</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>40.765484</td>\n",
       "      <td>-73.954739</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-26 20:19:48</td>\n",
       "      <td>2020-01-26 20:25:39</td>\n",
       "      <td>0.77</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.764421</td>\n",
       "      <td>-73.977569</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-16 13:17:34</td>\n",
       "      <td>2020-01-16 13:24:01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>40.775965</td>\n",
       "      <td>-73.987646</td>\n",
       "      <td>40.783961</td>\n",
       "      <td>-73.978632</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-30 20:20:32</td>\n",
       "      <td>2020-01-30 20:28:30</td>\n",
       "      <td>0.84</td>\n",
       "      <td>40.749914</td>\n",
       "      <td>-73.970443</td>\n",
       "      <td>40.756688</td>\n",
       "      <td>-73.972356</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tpep_pickup_datetime tpep_dropoff_datetime  trip_distance  latitude_pickup  \\\n",
       "0  2020-01-21 15:41:03   2020-01-21 15:54:32           0.71        40.749914   \n",
       "1  2020-01-29 21:06:16   2020-01-29 21:11:34           0.76        40.766948   \n",
       "2  2020-01-26 20:19:48   2020-01-26 20:25:39           0.77        40.758028   \n",
       "3  2020-01-16 13:17:34   2020-01-16 13:24:01           0.99        40.775965   \n",
       "4  2020-01-30 20:20:32   2020-01-30 20:28:30           0.84        40.749914   \n",
       "\n",
       "   longitude_pickup  latitude_dropoff  longitude_dropoff  fare_amount  extra  \\\n",
       "0        -73.970443         40.748575         -73.985156          9.0    0.0   \n",
       "1        -73.959635         40.765484         -73.954739          5.5    0.5   \n",
       "2        -73.977698         40.764421         -73.977569          5.5    0.5   \n",
       "3        -73.987646         40.783961         -73.978632          6.5    0.0   \n",
       "4        -73.970443         40.756688         -73.972356          6.5    0.5   \n",
       "\n",
       "   mta_tax  tip_amount  tolls_amount  improvement_surcharge  \\\n",
       "0      0.5        1.84           0.0                    0.3   \n",
       "1      0.5        0.00           0.0                    0.3   \n",
       "2      0.5        0.00           0.0                    0.3   \n",
       "3      0.5        1.96           0.0                    0.3   \n",
       "4      0.5        1.20           0.0                    0.3   \n",
       "\n",
       "   congestion_surcharge  airport_fee  total_amount  \n",
       "0                   2.5          NaN         14.14  \n",
       "1                   2.5          NaN          9.30  \n",
       "2                   2.5          NaN          9.30  \n",
       "3                   2.5          NaN         11.76  \n",
       "4                   2.5          NaN         11.50  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20869 entries, 0 to 21503\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   tpep_pickup_datetime   20869 non-null  datetime64[us]\n",
      " 1   tpep_dropoff_datetime  20869 non-null  datetime64[us]\n",
      " 2   trip_distance          20869 non-null  float64       \n",
      " 3   latitude_pickup        20869 non-null  float64       \n",
      " 4   longitude_pickup       20869 non-null  float64       \n",
      " 5   latitude_dropoff       20869 non-null  float64       \n",
      " 6   longitude_dropoff      20869 non-null  float64       \n",
      " 7   fare_amount            20869 non-null  float64       \n",
      " 8   extra                  20869 non-null  float64       \n",
      " 9   mta_tax                20869 non-null  float64       \n",
      " 10  tip_amount             20869 non-null  float64       \n",
      " 11  tolls_amount           20869 non-null  float64       \n",
      " 12  improvement_surcharge  20869 non-null  float64       \n",
      " 13  congestion_surcharge   19870 non-null  float64       \n",
      " 14  airport_fee            7930 non-null   float64       \n",
      " 15  total_amount           20869 non-null  float64       \n",
      "dtypes: datetime64[us](2), float64(14)\n",
      "memory usage: 2.7 MB\n"
     ]
    }
   ],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>latitude_pickup</th>\n",
       "      <th>longitude_pickup</th>\n",
       "      <th>latitude_dropoff</th>\n",
       "      <th>longitude_dropoff</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20869</td>\n",
       "      <td>20869</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "      <td>19870.000000</td>\n",
       "      <td>7930.000000</td>\n",
       "      <td>20869.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-01 23:23:35.520628</td>\n",
       "      <td>2022-05-01 23:40:27.647515</td>\n",
       "      <td>3.313074</td>\n",
       "      <td>40.753511</td>\n",
       "      <td>-73.966552</td>\n",
       "      <td>40.755856</td>\n",
       "      <td>-73.970556</td>\n",
       "      <td>15.666208</td>\n",
       "      <td>1.228502</td>\n",
       "      <td>0.490718</td>\n",
       "      <td>2.726723</td>\n",
       "      <td>0.456978</td>\n",
       "      <td>0.544477</td>\n",
       "      <td>2.293281</td>\n",
       "      <td>0.088115</td>\n",
       "      <td>22.794868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:20:28</td>\n",
       "      <td>2020-01-01 00:33:35</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.167234</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>-171.700000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.380000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>-188.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-28 14:22:00</td>\n",
       "      <td>2021-02-28 14:42:00</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>40.740439</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-04-30 10:32:36</td>\n",
       "      <td>2022-04-30 10:44:33</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-01 09:11:58</td>\n",
       "      <td>2023-07-01 09:26:38</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.961764</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 23:35:57</td>\n",
       "      <td>2024-08-31 23:44:08</td>\n",
       "      <td>55.600000</td>\n",
       "      <td>40.897932</td>\n",
       "      <td>-73.735554</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>7000.500000</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>33.440000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>7010.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.135789</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.045546</td>\n",
       "      <td>0.033581</td>\n",
       "      <td>0.037030</td>\n",
       "      <td>50.405868</td>\n",
       "      <td>1.513872</td>\n",
       "      <td>0.088052</td>\n",
       "      <td>3.153968</td>\n",
       "      <td>1.878248</td>\n",
       "      <td>0.350941</td>\n",
       "      <td>0.738788</td>\n",
       "      <td>0.321830</td>\n",
       "      <td>51.654328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tpep_pickup_datetime       tpep_dropoff_datetime  trip_distance  \\\n",
       "count                       20869                       20869   20869.000000   \n",
       "mean   2022-05-01 23:23:35.520628  2022-05-01 23:40:27.647515       3.313074   \n",
       "min           2020-01-01 00:20:28         2020-01-01 00:33:35       0.010000   \n",
       "25%           2021-02-28 14:22:00         2021-02-28 14:42:00       1.090000   \n",
       "50%           2022-04-30 10:32:36         2022-04-30 10:44:33       1.810000   \n",
       "75%           2023-07-01 09:11:58         2023-07-01 09:26:38       3.400000   \n",
       "max           2024-08-31 23:35:57         2024-08-31 23:44:08      55.600000   \n",
       "std                           NaN                         NaN       4.135789   \n",
       "\n",
       "       latitude_pickup  longitude_pickup  latitude_dropoff  longitude_dropoff  \\\n",
       "count     20869.000000      20869.000000      20869.000000       20869.000000   \n",
       "mean         40.753511        -73.966552         40.755856         -73.970556   \n",
       "min          40.576961        -74.167234         40.576961         -74.174002   \n",
       "25%          40.740439        -73.989845         40.740337         -73.989845   \n",
       "50%          40.758028        -73.977698         40.758028         -73.977698   \n",
       "75%          40.773633        -73.961764         40.775932         -73.959635   \n",
       "max          40.897932        -73.735554         40.899528         -73.726655   \n",
       "std           0.032516          0.045546          0.033581           0.037030   \n",
       "\n",
       "        fare_amount         extra       mta_tax    tip_amount  tolls_amount  \\\n",
       "count  20869.000000  20869.000000  20869.000000  20869.000000  20869.000000   \n",
       "mean      15.666208      1.228502      0.490718      2.726723      0.456978   \n",
       "min     -171.700000     -5.000000     -0.500000      0.000000    -15.380000   \n",
       "25%        7.200000      0.000000      0.500000      0.000000      0.000000   \n",
       "50%       10.700000      0.500000      0.500000      2.160000      0.000000   \n",
       "75%       17.500000      2.500000      0.500000      3.500000      0.000000   \n",
       "max     7000.500000     11.750000      0.800000     33.440000     40.000000   \n",
       "std       50.405868      1.513872      0.088052      3.153968      1.878248   \n",
       "\n",
       "       improvement_surcharge  congestion_surcharge  airport_fee  total_amount  \n",
       "count           20869.000000          19870.000000  7930.000000  20869.000000  \n",
       "mean                0.544477              2.293281     0.088115     22.794868  \n",
       "min                -1.000000             -2.500000    -1.250000   -188.080000  \n",
       "25%                 0.300000              2.500000     0.000000     12.600000  \n",
       "50%                 0.300000              2.500000     0.000000     16.800000  \n",
       "75%                 1.000000              2.500000     0.000000     24.500000  \n",
       "max                 1.000000              2.500000     1.250000   7010.850000  \n",
       "std                 0.350941              0.738788     0.321830     51.654328  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "This section downloads the relevant parquet files from the taxi website and creates a sample according to the sampling function of the Uber data. The sample dataframe is cleaned using the `get_and_clean_uber_data` function.\n",
    "\n",
    "* The `filter_uber_and_sample_monthly` function uses similar logic as the `sample_monthly` function, but it filters Uber trips before sampling\n",
    "\n",
    "* reads parquet files in the directory and filter Uber data each month, then creates a sample of each month and integrate the sample datasets into one dataset.\n",
    "\n",
    "* The `get_and_clean_uber_data` function takes a dataframe and returns a cleaned dataframe that:\n",
    "    * Filtered Uber rides\n",
    "    * Converted Location IDs to latitude lognitude coordinates\n",
    "    * Computed total fares for each ride\n",
    "    * Filtered rides that start and/or end within the New York bounding box\n",
    "    * Dropped columns that are irrelevant to later parts of the project\n",
    "    * Normalized column names and removed invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e29d34-cecb-4b05-a695-02de44a7cfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded fhvhv_tripdata_2024-01.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2024-02.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2024-03.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2024-04.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2024-05.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2024-06.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2024-07.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2024-08.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2023-01.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2023-02.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2023-03.parquet to uber_data\n",
      "Downloaded fhvhv_tripdata_2023-04.parquet to uber_data\n"
     ]
    }
   ],
   "source": [
    "uber_data_dir = \"uber_data\"\n",
    "download_parquet(uber_urls, uber_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8d281-4eed-4f0b-87e0-6189beef9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_uber_and_sample_monthly(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    \n",
    "    max_rows = 0\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        filtered_df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "        max_rows = max(max_rows, len(filtered_df))\n",
    "    sample_size = calculate_sample_size(max_rows)\n",
    "    print(f\"Sample size for all months: {sample_size}\")\n",
    "    \n",
    "    sampled_dataframes = []\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        filtered_df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "        sampled_df = filtered_df.sample(n=sample_size, random_state=30, replace=False)\n",
    "        sampled_dataframes.append(sampled_df)\n",
    "\n",
    "    if sampled_dataframes:\n",
    "        combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n",
    "        return combined_sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "322ce07f-c67c-41a2-8998-2e2421aef2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for all months: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/llw7py457rl4zlvsw5ymljw80000gn/T/ipykernel_32667/359863056.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "sampled_uber_df = filter_uber_and_sample_monthly(uber_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca549f-a7ec-4b73-b534-52abd1702717",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_uber_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dca0d9f7-3607-4feb-ac93-810c7b3770ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21504 entries, 0 to 21503\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   hvfhs_license_num     21504 non-null  object        \n",
      " 1   dispatching_base_num  21504 non-null  object        \n",
      " 2   originating_base_num  15516 non-null  object        \n",
      " 3   request_datetime      21504 non-null  datetime64[us]\n",
      " 4   on_scene_datetime     15519 non-null  datetime64[us]\n",
      " 5   pickup_datetime       21504 non-null  datetime64[us]\n",
      " 6   dropoff_datetime      21504 non-null  datetime64[us]\n",
      " 7   PULocationID          21504 non-null  int64         \n",
      " 8   DOLocationID          21504 non-null  int64         \n",
      " 9   trip_miles            21504 non-null  float64       \n",
      " 10  trip_time             21504 non-null  int64         \n",
      " 11  base_passenger_fare   21504 non-null  float64       \n",
      " 12  tolls                 21504 non-null  float64       \n",
      " 13  bcf                   21504 non-null  float64       \n",
      " 14  sales_tax             21504 non-null  float64       \n",
      " 15  congestion_surcharge  21504 non-null  float64       \n",
      " 16  airport_fee           15780 non-null  float64       \n",
      " 17  tips                  21504 non-null  float64       \n",
      " 18  driver_pay            21504 non-null  float64       \n",
      " 19  shared_request_flag   21504 non-null  object        \n",
      " 20  shared_match_flag     21504 non-null  object        \n",
      " 21  access_a_ride_flag    21504 non-null  object        \n",
      " 22  wav_request_flag      21504 non-null  object        \n",
      " 23  wav_match_flag        21504 non-null  object        \n",
      "dtypes: datetime64[us](4), float64(9), int64(3), object(8)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "sampled_uber_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9898b-c1d5-420e-954f-bdf0a9e6fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(df):\n",
    "    fare_columns = [\"base_passenger_fare\", \"tolls\", \"bcf\", \"sales_tax\", \"congestion_surcharge\", \"airport_fee\"]\n",
    "    columns_to_keep = [\"request_datetime\", \"on_scene_datetime\", \"pickup_datetime\", \"dropoff_datetime\",\n",
    "                       \"pickup_lat\", \"pickup_lon\", \"dropoff_lat\", \"dropoff_lon\", \"trip_miles\",\n",
    "                       \"total_fare\", \"tips\"]\n",
    "    # filter uber data\n",
    "    df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "\n",
    "    # convert LocationID to coordinates\n",
    "    df[[\"pickup_lat\", \"pickup_lon\"]] = df[\"PULocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "    df[[\"dropoff_lat\", \"dropoff_lon\"]] = df[\"DOLocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "    \n",
    "    # remove invalid locations & 0 mile trips\n",
    "    df = df.dropna(subset=[\"pickup_lat\", \"pickup_lon\", \"dropoff_lat\", \"dropoff_lon\"])\n",
    "    df = df[df[\"trip_miles\"] != 0]\n",
    "    \n",
    "    # filter trips within the bounding box\n",
    "    ((min_lat, min_lon), (max_lat, max_lon)) = NEW_YORK_BOX_COORDS\n",
    "    pickup_in_box = (\n",
    "        (df[\"pickup_lat\"] >= min_lat) & (df[\"pickup_lat\"] <= max_lat) &\n",
    "        (df[\"pickup_lon\"] >= min_lon) & (df[\"pickup_lon\"] <= max_lon)\n",
    "    )\n",
    "    dropoff_in_box = (\n",
    "        (df[\"dropoff_lat\"] >= min_lat) & (df[\"dropoff_lat\"] <= max_lat) &\n",
    "        (df[\"dropoff_lon\"] >= min_lon) & (df[\"dropoff_lon\"] <= max_lon)\n",
    "    )\n",
    "    df = df[pickup_in_box & dropoff_in_box]\n",
    "        \n",
    "    # compute total fare\n",
    "    df[fare_columns] = df[fare_columns].fillna(0)\n",
    "    df[\"total_fare\"] = df[fare_columns].sum(axis=1)\n",
    "\n",
    "    df = df[columns_to_keep]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "b8bf7dbd-49a1-49c1-b6d4-1b6767855f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uber_data = get_and_clean_uber_data(sampled_uber_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "7902ce9d-313f-4c3e-9cb6-0e87cb1f4256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pickup_lat</th>\n",
       "      <th>pickup_lon</th>\n",
       "      <th>dropoff_lat</th>\n",
       "      <th>dropoff_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>total_fare</th>\n",
       "      <th>tips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-25 09:31:19</td>\n",
       "      <td>2021-03-25 09:32:12</td>\n",
       "      <td>2021-03-25 09:34:06</td>\n",
       "      <td>2021-03-25 09:39:19</td>\n",
       "      <td>40.620924</td>\n",
       "      <td>-73.956824</td>\n",
       "      <td>40.620924</td>\n",
       "      <td>-73.956824</td>\n",
       "      <td>1.05</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-24 12:18:27</td>\n",
       "      <td>2021-03-24 12:23:05</td>\n",
       "      <td>2021-03-24 12:24:54</td>\n",
       "      <td>2021-03-24 12:53:16</td>\n",
       "      <td>40.763352</td>\n",
       "      <td>-73.868395</td>\n",
       "      <td>40.664003</td>\n",
       "      <td>-73.910258</td>\n",
       "      <td>11.59</td>\n",
       "      <td>34.59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-08 12:23:10</td>\n",
       "      <td>2021-03-08 12:25:52</td>\n",
       "      <td>2021-03-08 12:25:52</td>\n",
       "      <td>2021-03-08 12:32:20</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.946510</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>2.19</td>\n",
       "      <td>18.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-04 22:14:05</td>\n",
       "      <td>2021-03-04 22:19:17</td>\n",
       "      <td>2021-03-04 22:19:38</td>\n",
       "      <td>2021-03-04 22:25:14</td>\n",
       "      <td>40.685634</td>\n",
       "      <td>-73.986114</td>\n",
       "      <td>40.685634</td>\n",
       "      <td>-73.986114</td>\n",
       "      <td>0.83</td>\n",
       "      <td>8.45</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-16 05:03:32</td>\n",
       "      <td>2021-03-16 05:10:15</td>\n",
       "      <td>2021-03-16 05:12:15</td>\n",
       "      <td>2021-03-16 05:26:09</td>\n",
       "      <td>40.833990</td>\n",
       "      <td>-73.885900</td>\n",
       "      <td>40.807347</td>\n",
       "      <td>-73.916822</td>\n",
       "      <td>2.71</td>\n",
       "      <td>16.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-03-04 16:27:53</td>\n",
       "      <td>2021-03-04 16:30:52</td>\n",
       "      <td>2021-03-04 16:32:20</td>\n",
       "      <td>2021-03-04 16:43:20</td>\n",
       "      <td>40.841708</td>\n",
       "      <td>-73.941399</td>\n",
       "      <td>40.837827</td>\n",
       "      <td>-73.926158</td>\n",
       "      <td>2.28</td>\n",
       "      <td>10.83</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-03-18 20:56:27</td>\n",
       "      <td>2021-03-18 21:00:19</td>\n",
       "      <td>2021-03-18 21:01:06</td>\n",
       "      <td>2021-03-18 21:13:02</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.990458</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>2.54</td>\n",
       "      <td>18.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-03-13 00:35:54</td>\n",
       "      <td>2021-03-13 00:38:16</td>\n",
       "      <td>2021-03-13 00:40:16</td>\n",
       "      <td>2021-03-13 00:49:16</td>\n",
       "      <td>40.640590</td>\n",
       "      <td>-73.976199</td>\n",
       "      <td>40.641886</td>\n",
       "      <td>-74.004653</td>\n",
       "      <td>1.39</td>\n",
       "      <td>6.29</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-03-17 15:38:12</td>\n",
       "      <td>2021-03-17 15:41:44</td>\n",
       "      <td>2021-03-17 15:43:26</td>\n",
       "      <td>2021-03-17 16:24:06</td>\n",
       "      <td>40.620924</td>\n",
       "      <td>-73.956824</td>\n",
       "      <td>40.766238</td>\n",
       "      <td>-73.995135</td>\n",
       "      <td>12.48</td>\n",
       "      <td>61.31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-03-13 12:23:41</td>\n",
       "      <td>2021-03-13 12:26:47</td>\n",
       "      <td>2021-03-13 12:27:54</td>\n",
       "      <td>2021-03-13 12:45:38</td>\n",
       "      <td>40.612218</td>\n",
       "      <td>-73.995259</td>\n",
       "      <td>40.580922</td>\n",
       "      <td>-73.961217</td>\n",
       "      <td>4.08</td>\n",
       "      <td>20.27</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     request_datetime   on_scene_datetime     pickup_datetime  \\\n",
       "0 2021-03-25 09:31:19 2021-03-25 09:32:12 2021-03-25 09:34:06   \n",
       "1 2021-03-24 12:18:27 2021-03-24 12:23:05 2021-03-24 12:24:54   \n",
       "2 2021-03-08 12:23:10 2021-03-08 12:25:52 2021-03-08 12:25:52   \n",
       "3 2021-03-04 22:14:05 2021-03-04 22:19:17 2021-03-04 22:19:38   \n",
       "4 2021-03-16 05:03:32 2021-03-16 05:10:15 2021-03-16 05:12:15   \n",
       "5 2021-03-04 16:27:53 2021-03-04 16:30:52 2021-03-04 16:32:20   \n",
       "6 2021-03-18 20:56:27 2021-03-18 21:00:19 2021-03-18 21:01:06   \n",
       "7 2021-03-13 00:35:54 2021-03-13 00:38:16 2021-03-13 00:40:16   \n",
       "8 2021-03-17 15:38:12 2021-03-17 15:41:44 2021-03-17 15:43:26   \n",
       "9 2021-03-13 12:23:41 2021-03-13 12:26:47 2021-03-13 12:27:54   \n",
       "\n",
       "     dropoff_datetime  pickup_lat  pickup_lon  dropoff_lat  dropoff_lon  \\\n",
       "0 2021-03-25 09:39:19   40.620924  -73.956824    40.620924   -73.956824   \n",
       "1 2021-03-24 12:53:16   40.763352  -73.868395    40.664003   -73.910258   \n",
       "2 2021-03-08 12:32:20   40.775932  -73.946510    40.756729   -73.965146   \n",
       "3 2021-03-04 22:25:14   40.685634  -73.986114    40.685634   -73.986114   \n",
       "4 2021-03-16 05:26:09   40.833990  -73.885900    40.807347   -73.916822   \n",
       "5 2021-03-04 16:43:20   40.841708  -73.941399    40.837827   -73.926158   \n",
       "6 2021-03-18 21:13:02   40.740337  -73.990458    40.756729   -73.965146   \n",
       "7 2021-03-13 00:49:16   40.640590  -73.976199    40.641886   -74.004653   \n",
       "8 2021-03-17 16:24:06   40.620924  -73.956824    40.766238   -73.995135   \n",
       "9 2021-03-13 12:45:38   40.612218  -73.995259    40.580922   -73.961217   \n",
       "\n",
       "   trip_miles  total_fare  tips  \n",
       "0        1.05        8.85   0.0  \n",
       "1       11.59       34.59   0.0  \n",
       "2        2.19       18.40   0.0  \n",
       "3        0.83        8.45   3.0  \n",
       "4        2.71       16.80   0.0  \n",
       "5        2.28       10.83   0.0  \n",
       "6        2.54       18.02   0.0  \n",
       "7        1.39        6.29   1.0  \n",
       "8       12.48       61.31   0.0  \n",
       "9        4.08       20.27   3.0  "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "8ac89430-73c0-42cf-b690-d9c821e2184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20650 entries, 0 to 21503\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   request_datetime   20650 non-null  datetime64[us]\n",
      " 1   on_scene_datetime  20650 non-null  datetime64[us]\n",
      " 2   pickup_datetime    20650 non-null  datetime64[us]\n",
      " 3   dropoff_datetime   20650 non-null  datetime64[us]\n",
      " 4   pickup_lat         20650 non-null  float64       \n",
      " 5   pickup_lon         20650 non-null  float64       \n",
      " 6   dropoff_lat        20650 non-null  float64       \n",
      " 7   dropoff_lon        20650 non-null  float64       \n",
      " 8   trip_miles         20650 non-null  float64       \n",
      " 9   total_fare         20650 non-null  float64       \n",
      " 10  tips               20650 non-null  float64       \n",
      "dtypes: datetime64[us](4), float64(7)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    all_urls = get_all_urls_from_tlc_page(TLC_URL)\n",
    "    all_parquet_urls = find_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "In this section, we processed the weather data and creates dataframes with hourly and daily granularity information retaining relevant information only.\n",
    "*  `get_all_weather_csvs` returns the weather csv files in the directory\n",
    "*  `clean_month_weather_data_hourly` takes csv files and returns a dataframe that contains hourly precipation and wind speed information\n",
    "*  `clean_month_weather_data_daily` takes csv files and returns a dataframe that contains daily precipation, wind speed, and snowfall information. The function fills in values according to the data description for better data processing later\n",
    "*  `load_and_clean_weather_data` concatnates all daily dataframes and all hourly dataframes into two large dataframes that contains all daily weather data and all hourly data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    weather_csvs = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "    return weather_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    \n",
    "    df[\"date\"] = pd.to_datetime(df[\"DATE\"])\n",
    "    df[\"HourlyPrecipitation\"] = (df[\"HourlyPrecipitation\"]\n",
    "        .replace(\"T\", \"0.005\")  # Replace 'T' (trace) with a small float\n",
    "        .str.extract(r\"([\\d\\.]+)\")  # Extract numeric part, ignore non-numeric\n",
    "        .astype(float)  # Convert to float\n",
    "    )\n",
    "\n",
    "    columns = [\"date\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]\n",
    "    df = df[columns]\n",
    "    \n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"DATE\"])\n",
    "\n",
    "    df[\"DailyPrecipitation\"] = (df[\"DailyPrecipitation\"]\n",
    "            .replace(\"T\", \"0.005\")  \n",
    "            .str.extract(r\"([\\d\\.]+)\")  \n",
    "            .astype(float)  \n",
    "    )\n",
    "    df[\"DailySnowfall\"] = (df[\"DailySnowfall\"]\n",
    "            .replace(\"T\", \"0.005\")  \n",
    "            .str.extract(r\"([\\d\\.]+)\")  \n",
    "            .astype(float) \n",
    "    )\n",
    "    df[\"DailySnowDepth\"] = (df[\"DailySnowDepth\"].replace(\"T\", \"0.005\").astype(float))\n",
    "    \n",
    "    columns = [\"date\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\",\n",
    "               \"DailySnowfall\", \"DailySnowDepth\"]\n",
    "    df = df[columns]\n",
    "\n",
    "    df = df.dropna(subset=[\"date\",\"DailyPrecipitation\", \"DailyAverageWindSpeed\",\n",
    "               \"DailySnowfall\", \"DailySnowDepth\"])\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "f7cd53a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "c8230fe2-aee4-426e-a197-142c90347b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourlyprecipitation</th>\n",
       "      <th>hourlywindspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  hourlyprecipitation  hourlywindspeed\n",
       "0 2020-01-01 00:51:00                  0.0              8.0\n",
       "1 2020-01-01 01:51:00                  0.0              8.0\n",
       "2 2020-01-01 02:51:00                  0.0             14.0\n",
       "3 2020-01-01 03:51:00                  0.0             11.0\n",
       "4 2020-01-01 04:51:00                  0.0              6.0"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 56098 entries, 0 to 11638\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   date                 56098 non-null  datetime64[ns]\n",
      " 1   hourlyprecipitation  56098 non-null  float64       \n",
      " 2   hourlywindspeed      56098 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourlyprecipitation</th>\n",
       "      <th>hourlywindspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56098</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-29 21:14:19.618881024</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>4.537238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-18 19:01:45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-28 01:21:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-15 05:39:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>2237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>13.883208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  hourlyprecipitation  hourlywindspeed\n",
       "count                          56098         56098.000000     56098.000000\n",
       "mean   2022-05-29 21:14:19.618881024             0.010841         4.537238\n",
       "min              2020-01-01 00:51:00             0.000000         0.000000\n",
       "25%              2021-03-18 19:01:45             0.000000         0.000000\n",
       "50%              2022-05-28 01:21:00             0.000000         5.000000\n",
       "75%              2023-08-15 05:39:00             0.000000         7.000000\n",
       "max              2024-10-22 18:51:00             3.470000      2237.000000\n",
       "std                              NaN             0.056735        13.883208"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dailyprecipitation</th>\n",
       "      <th>dailyaveragewindspeed</th>\n",
       "      <th>dailysnowfall</th>\n",
       "      <th>dailysnowdepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11335</th>\n",
       "      <td>2022-12-22 23:59:00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11389</th>\n",
       "      <td>2022-12-23 23:59:00</td>\n",
       "      <td>1.83</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>2022-12-24 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11439</th>\n",
       "      <td>2022-12-25 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11464</th>\n",
       "      <td>2022-12-26 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11489</th>\n",
       "      <td>2022-12-27 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516</th>\n",
       "      <td>2022-12-28 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>2022-12-29 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11566</th>\n",
       "      <td>2022-12-30 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11637</th>\n",
       "      <td>2022-12-31 23:59:00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  dailyprecipitation  dailyaveragewindspeed  \\\n",
       "11335 2022-12-22 23:59:00                0.23                    8.6   \n",
       "11389 2022-12-23 23:59:00                1.83                   10.0   \n",
       "11414 2022-12-24 23:59:00                0.00                   11.3   \n",
       "11439 2022-12-25 23:59:00                0.00                    8.5   \n",
       "11464 2022-12-26 23:59:00                0.00                    7.1   \n",
       "11489 2022-12-27 23:59:00                0.00                    5.5   \n",
       "11516 2022-12-28 23:59:00                0.00                    5.1   \n",
       "11541 2022-12-29 23:59:00                0.00                    6.1   \n",
       "11566 2022-12-30 23:59:00                0.00                    2.9   \n",
       "11637 2022-12-31 23:59:00                0.28                    1.8   \n",
       "\n",
       "       dailysnowfall  dailysnowdepth  \n",
       "11335          0.000             0.0  \n",
       "11389          0.005             0.0  \n",
       "11414          0.000             0.0  \n",
       "11439          0.000             0.0  \n",
       "11464          0.000             0.0  \n",
       "11489          0.000             0.0  \n",
       "11516          0.000             0.0  \n",
       "11541          0.000             0.0  \n",
       "11566          0.000             0.0  \n",
       "11637          0.000             0.0  "
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1692 entries, 24 to 11637\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   date                   1692 non-null   datetime64[ns]\n",
      " 1   dailyprecipitation     1692 non-null   float64       \n",
      " 2   dailyaveragewindspeed  1692 non-null   float64       \n",
      " 3   dailysnowfall          1692 non-null   float64       \n",
      " 4   dailysnowdepth         1692 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 79.3 KB\n"
     ]
    }
   ],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dailyprecipitation</th>\n",
       "      <th>dailyaveragewindspeed</th>\n",
       "      <th>dailysnowfall</th>\n",
       "      <th>dailysnowdepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1692</td>\n",
       "      <td>1692.000000</td>\n",
       "      <td>1692.000000</td>\n",
       "      <td>1692.000000</td>\n",
       "      <td>1692.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-06-11 16:53:28.085106176</td>\n",
       "      <td>0.145018</td>\n",
       "      <td>5.000355</td>\n",
       "      <td>0.040721</td>\n",
       "      <td>0.160195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 23:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-04-05 17:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-06-19 11:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-16 05:59:00</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>6.325000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-21 23:59:00</td>\n",
       "      <td>7.130000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420440</td>\n",
       "      <td>2.339679</td>\n",
       "      <td>0.502493</td>\n",
       "      <td>1.059881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  dailyprecipitation  \\\n",
       "count                           1692         1692.000000   \n",
       "mean   2022-06-11 16:53:28.085106176            0.145018   \n",
       "min              2020-01-01 23:59:00            0.000000   \n",
       "25%              2021-04-05 17:59:00            0.000000   \n",
       "50%              2022-06-19 11:59:00            0.000000   \n",
       "75%              2023-08-16 05:59:00            0.060000   \n",
       "max              2024-10-21 23:59:00            7.130000   \n",
       "std                              NaN            0.420440   \n",
       "\n",
       "       dailyaveragewindspeed  dailysnowfall  dailysnowdepth  \n",
       "count            1692.000000    1692.000000     1692.000000  \n",
       "mean                5.000355       0.040721        0.160195  \n",
       "min                 0.600000       0.000000        0.000000  \n",
       "25%                 3.200000       0.000000        0.000000  \n",
       "50%                 4.600000       0.000000        0.000000  \n",
       "75%                 6.325000       0.000000        0.000000  \n",
       "max                14.200000      14.800000       14.000000  \n",
       "std                 2.339679       0.502493        1.059881  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATETIME,\n",
    "    daily_precipitation FLOAT,\n",
    "    daily_average_wind_speed FLOAT,\n",
    "    daily_snowfall FLOAT,\n",
    "    daily_snow_depth FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips(\n",
    "     tpep_pickup_datetime DATATIME, \n",
    "     tpep_dropoff_datetime DATATIME,\n",
    "     trip_distance FLOAT, \n",
    "     latitude_pickup FLOAT, \n",
    "     longitude_pickup FLOAT, \n",
    "     latitude_dropoff FLOAT, \n",
    "     longitude_dropoff FLOAT, \n",
    "     fare_amount FLOAT, \n",
    "     extra FLOAT, \n",
    "     mta_tax FLOAT, \n",
    "     tip_amount FLOAT, \n",
    "     tolls_amount FLOAT, \n",
    "     improvement_surcharge FLOAT, \n",
    "     congestion_surcharge FLOAT, \n",
    "     airport_fee FLOAT, \n",
    "     total_amount FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips(\n",
    "    request_datetime DATETIME,\n",
    "    on_scene_datetime DATETIME,\n",
    "    pickup_datetime DATETIME,\n",
    "    dropoff_datetime DATETIME,\n",
    "    pickup_lat FLOAT,\n",
    "    pickup_lon FLOAT,\n",
    "    dropoff_lat FLOAT,\n",
    "    dropoff_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    total_fare FLOAT,\n",
    "    tips FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    with open(DATABASE_SCHEMA_FILE, \"r\") as f:\n",
    "        schema_sql = f.read()\n",
    "    schema_stmts = [stmt.strip() for stmt in schema_sql.split(\";\") if stmt.strip()]\n",
    "    for stmt in schema_stmts:\n",
    "        connection.execute(db.text(stmt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
