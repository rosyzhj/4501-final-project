{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"weather_data\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones\n",
    "In this section, we loaded the taxi shapefile that corresponds location IDs to geographical latitudes and logitudes. \n",
    "* The `load_taxi_zones` function reads the shapefile and use GeoPandas to read the file\n",
    "* The `lookup_coords_for_taxi_zone_id` function takes location IDs and the loaded shapefile and returns a tuple of latitude and logitude\n",
    "* The `make_loc_id_coords_dict` creates a dictionary of location IDs and coordinates that will be used in data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    taxi_zones = gpd.read_file(shapefile)\n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5296e8e3-e831-4d68-b1a2-8af4d49c949d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Projected CRS: EPSG:2263>\n",
       "Name: NAD83 / New York Long Island (ftUS)\n",
       "Axis Info [cartesian]:\n",
       "- X[east]: Easting (US survey foot)\n",
       "- Y[north]: Northing (US survey foot)\n",
       "Area of Use:\n",
       "- name: United States (USA) - New York - counties of Bronx; Kings; Nassau; New York; Queens; Richmond; Suffolk.\n",
       "- bounds: (-74.26, 40.47, -71.8, 41.3)\n",
       "Coordinate Operation:\n",
       "- name: SPCS83 New York Long Island zone (US survey foot)\n",
       "- method: Lambert Conic Conformal (2SP)\n",
       "Datum: North American Datum 1983\n",
       "- Ellipsoid: GRS 1980\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "taxi_zones.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a924269c-1348-4110-ba72-326f2f258264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones=taxi_zones):\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones[\"LocationID\"] == zone_loc_id]\n",
    "    centroid = zone.geometry.centroid.iloc[0]\n",
    "    centroid_geo = gpd.GeoSeries([centroid], crs=loaded_taxi_zones.crs).to_crs(epsg=CRS).iloc[0]\n",
    "\n",
    "    latitude = centroid_geo.y\n",
    "    longitude = centroid_geo.x\n",
    "\n",
    "    return (latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "dae65882-f709-4348-80a0-6276cd04eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loc_id_coords_dict(loaded_taxi_zones):\n",
    "    id_coords_dict = {}\n",
    "    for loc_id in loaded_taxi_zones[\"LocationID\"]:\n",
    "        id_coords_dict[loc_id] = lookup_coords_for_taxi_zone_id(loc_id, loaded_taxi_zones)\n",
    "\n",
    "    return id_coords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "90e63ccc-1ff3-46cf-8252-d402e9cf1a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ID_COORDS_DICT = make_loc_id_coords_dict(taxi_zones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population):\n",
    "    confidence_level = 0.95\n",
    "    margin_of_error = 0.05\n",
    "    proportion = 0.5\n",
    "\n",
    "    z_score = norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "\n",
    "    # Cochranâ€™s\n",
    "    n = (z_score**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population\n",
    "    n_adj = n / (1 + (n - 1) / population)\n",
    "    \n",
    "    return int(round(n_adj)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions\n",
    "* `get_all_urls_from_taxi_page` fetches information on the taxi page and finds all \"Yellow Taxi Trip Records\" urls and \"High Volume For-Hire Vehicle Trip Records\" urls\n",
    "* `find_parquet_urls` uses regex to filter the urls that ends with \".parquet\" to make sure that the urls are parquet files\n",
    "* `download_parquet` creates a directory and downloads relevant parquets into the directory\n",
    "* `get_and_clean_month` filters the urls that are from January 2020 to August 2024\n",
    "* `sample_monthly` function reads all the parquet files in a directory, finds the file with largest number of rows and computes the sample size using the \"maximum population\". Next it creates samples for all files using the computed sample size and combine them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    response = requests.get(taxi_page)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    yellow_tags = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    "    fhvhv_tags = soup.find_all(\"a\", attrs={\"title\": \"High Volume For-Hire Vehicle Trip Records\"})\n",
    "\n",
    "    yellow_urls = [a[\"href\"].strip() for a in yellow_tags]\n",
    "    fhvhv_urls = [a[\"href\"].strip() for a in fhvhv_tags]\n",
    "    \n",
    "    return yellow_urls, fhvhv_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parquet_urls(urls):\n",
    "    pattern = re.compile(r\"\\.parquet$\")\n",
    "    parquet_urls = [url for url in urls if pattern.search(url)]    \n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b77c3ef5-812f-45db-bed1-d9f8ef52269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parquet(urls, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for url in urls:\n",
    "        filename = os.path.basename(url)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        if os.path.exists(output_path):\n",
    "            continue        \n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded {filename} to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(urls):\n",
    "    pattern = re.compile(r\"(202[0-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\")\n",
    "    cleaned_urls = [url for url in urls if pattern.search(url)]\n",
    "    return cleaned_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a2c60ef2-3ab2-44f2-938e-05cbf8868f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_to_df(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".parquet\")]\n",
    "    all_dataframe = []\n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_parquet(file)\n",
    "        all_dataframe.append(df)\n",
    "    if all_dataframe:\n",
    "        combined_df = pd.concat(all_dataframe, ignore_index=True)\n",
    "        return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "116a75c2-0bfb-4149-9548-47a35f3f2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parquet_column(file_path, columns_to_keep):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    cleaned_df = df[columns_to_keep]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7e11d08-6d62-4acd-bf15-3e1fe5ec5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_urls, fhvhv_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "taxi_parquet = find_parquet_urls(yellow_urls)\n",
    "uber_parquet = find_parquet_urls(fhvhv_urls)\n",
    "taxi_urls = get_and_clean_month(taxi_parquet)\n",
    "uber_urls = get_and_clean_month(uber_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7cd5edd0-7191-4066-94b7-9bdd11c3e7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_sample_size(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".parquet\")]\n",
    "\n",
    "    max_rows = 0\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(file)\n",
    "            max_rows = max(max_rows, df.shape[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}: {e}\")\n",
    "            continue  \n",
    "\n",
    "    sample_size = calculate_sample_size(max_rows)\n",
    "    print(f\"Sample size for all months: {sample_size}\")\n",
    "    return sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c6396fb8-bea9-4db3-a918-59eeeee34518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_monthly(directory: str, sample_size: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Samples a specified number of rows from each Parquet file in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing Parquet files.\n",
    "        sample_size (int): Number of rows to sample from each file.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of sampled DataFrames for each month.\n",
    "    \"\"\"\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".parquet\")]\n",
    "    sampled_dataframes = []\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        sampled_df = df.sample(n=sample_size, random_state=30, replace=False)\n",
    "        sampled_dataframes.append(sampled_df)\n",
    "\n",
    "    print(\"Finished sampling\")\n",
    "    return sampled_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cb0a333f-84b4-40c6-86e7-f2d56ecba837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taxi_data_dir = \"taxi_data\"\n",
    "download_parquet(taxi_urls, taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9afdd08-4deb-4dfb-ae9a-26c27fd35df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taxi data sample size: 384\n"
     ]
    }
   ],
   "source": [
    "taxi_sample_size = calculate_monthly_sample_size(taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4c735ed-ff31-4319-9b72-368fa98da83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished sampling\n"
     ]
    }
   ],
   "source": [
    "taxi_monthly_sampled = sample_monthly(taxi_data_dir, taxi_sample_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "df2cf664-5d59-4551-89dc-1478541907f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxidata(dataframes: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans a list of taxi DataFrames, resamples them based on taxi_sample_size,\n",
    "    and concatenates them into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframes (List[pd.DataFrame]): List of taxi DataFrames to be cleaned and sampled.\n",
    "        taxi_sample_size (int): Number of samples to keep for each DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and concatenated DataFrame of sampled taxi data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Cleaning and sampling the list of dataframes...\")\n",
    "\n",
    "        cleaned_sampled_dataframes = []\n",
    "\n",
    "        for dataframe in dataframes:\n",
    "            if not isinstance(dataframe, pd.DataFrame):\n",
    "                raise ValueError(\"Each item in the list must be a Pandas DataFrame.\")\n",
    "\n",
    "            # Look up latitude and longitude (get those coordinates)\n",
    "            dataframe[[\"latitude_pickup\", \"longitude_pickup\"]] = dataframe[\"PULocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "            dataframe[[\"latitude_dropoff\", \"longitude_dropoff\"]] = dataframe[\"DOLocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "\n",
    "            # Remove invalid location IDs and where distance is 0\n",
    "            dataframe = dataframe.dropna(subset=[\"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\"])\n",
    "            dataframe = dataframe[dataframe[\"trip_distance\"] > 0]\n",
    "\n",
    "            # Normalize column names\n",
    "            dataframe.columns = [col.lower().replace(\" \", \"_\") for col in dataframe.columns]\n",
    "\n",
    "            # Normalize and use appropriate column types\n",
    "            dataframe[\"tpep_pickup_datetime\"] = pd.to_datetime(dataframe[\"tpep_pickup_datetime\"])\n",
    "            dataframe[\"tpep_dropoff_datetime\"] = pd.to_datetime(dataframe[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "            # Filter trips within New York bounding box\n",
    "            lat_min, lon_min = NEW_YORK_BOX_COORDS[0]\n",
    "            lat_max, lon_max = NEW_YORK_BOX_COORDS[1]\n",
    "            dataframe = dataframe[\n",
    "                (dataframe[\"latitude_pickup\"].between(lat_min, lat_max)) &\n",
    "                (dataframe[\"longitude_pickup\"].between(lon_min, lon_max)) &\n",
    "                (dataframe[\"latitude_dropoff\"].between(lat_min, lat_max)) &\n",
    "                (dataframe[\"longitude_dropoff\"].between(lon_min, lon_max))\n",
    "            ]\n",
    "\n",
    "            # Resample the cleaned DataFrame\n",
    "            sampled_dataframe = dataframe.sample(n=taxi_sample_size, random_state=30, replace=False)\n",
    "            cleaned_sampled_dataframes.append(sampled_dataframe)\n",
    "\n",
    "        # Concatenate all sampled DataFrames\n",
    "        if cleaned_sampled_dataframes:\n",
    "            final_dataframe = pd.concat(cleaned_sampled_dataframes, ignore_index=True)\n",
    "\n",
    "            # Combine \"airport_fee\" with \"Airport_fee\"\n",
    "            if \"Airport_fee\" in final_dataframe.columns:\n",
    "                final_dataframe[\"airport_fee\"] = final_dataframe[\"Airport_fee\"].combine_first(final_dataframe[\"airport_fee\"])\n",
    "                final_dataframe.drop(columns=[\"Airport_fee\"], inplace=True)\n",
    "            final_dataframe[\"airport_fee\"] = final_dataframe[\"airport_fee\"].fillna(0)\n",
    "\n",
    "            # Calculate fares\n",
    "            surcharge_columns = [\"extra\", \"improvement_surcharge\", \"congestion_surcharge\", \"airport_fee\"]\n",
    "            final_dataframe[surcharge_columns] = final_dataframe[surcharge_columns].fillna(0)\n",
    "            final_dataframe[\"total_surcharge\"] = final_dataframe[surcharge_columns].sum(axis=1)\n",
    "            final_dataframe[\"total_money\"] = (final_dataframe[\"fare_amount\"] \n",
    "                                            + final_dataframe[\"total_surcharge\"] \n",
    "                                            + final_dataframe[\"mta_tax\"] \n",
    "                                            + final_dataframe[\"tolls_amount\"])\n",
    "\n",
    "            # Remove unnecessary columns\n",
    "            columns_to_keep = [\n",
    "                \"tpep_pickup_datetime\",\n",
    "                \"trip_distance\",\n",
    "                \"latitude_pickup\", \"longitude_pickup\", \"latitude_dropoff\", \"longitude_dropoff\",\n",
    "                \"fare_amount\", \"total_surcharge\", \"mta_tax\", \"tolls_amount\", \"total_money\", \n",
    "                \"tip_amount\"\n",
    "            ]\n",
    "            final_dataframe = final_dataframe[columns_to_keep]\n",
    "            return final_dataframe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the dataframes: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "508ab469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and sampling the list of dataframes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/llw7py457rl4zlvsw5ymljw80000gn/T/ipykernel_54357/2780130706.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_dataframe = pd.concat(cleaned_sampled_dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_and_clean_taxidata(taxi_monthly_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>latitude_pickup</th>\n",
       "      <th>longitude_pickup</th>\n",
       "      <th>latitude_dropoff</th>\n",
       "      <th>longitude_dropoff</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_surcharge</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_money</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-23 07:44:31</td>\n",
       "      <td>2.90</td>\n",
       "      <td>40.780436</td>\n",
       "      <td>-73.957012</td>\n",
       "      <td>40.753513</td>\n",
       "      <td>-73.988787</td>\n",
       "      <td>16.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.8</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-07 15:52:08</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40.756688</td>\n",
       "      <td>-73.972356</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-09 11:20:16</td>\n",
       "      <td>1.50</td>\n",
       "      <td>40.723888</td>\n",
       "      <td>-74.001538</td>\n",
       "      <td>40.735035</td>\n",
       "      <td>-74.008984</td>\n",
       "      <td>12.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-07 19:14:34</td>\n",
       "      <td>1.52</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>40.778766</td>\n",
       "      <td>-73.951010</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>3.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-21 07:33:32</td>\n",
       "      <td>1.98</td>\n",
       "      <td>40.780436</td>\n",
       "      <td>-73.957012</td>\n",
       "      <td>40.801169</td>\n",
       "      <td>-73.937346</td>\n",
       "      <td>12.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tpep_pickup_datetime  trip_distance  latitude_pickup  longitude_pickup  \\\n",
       "0  2023-06-23 07:44:31           2.90        40.780436        -73.957012   \n",
       "1  2023-06-07 15:52:08           0.95        40.756688        -73.972356   \n",
       "2  2023-06-09 11:20:16           1.50        40.723888        -74.001538   \n",
       "3  2023-06-07 19:14:34           1.52        40.756729        -73.965146   \n",
       "4  2023-06-21 07:33:32           1.98        40.780436        -73.957012   \n",
       "\n",
       "   latitude_dropoff  longitude_dropoff  fare_amount  total_surcharge  mta_tax  \\\n",
       "0         40.753513         -73.988787         16.3              6.0      0.5   \n",
       "1         40.756729         -73.965146         10.7              3.5      0.5   \n",
       "2         40.735035         -74.008984         12.1              3.5      0.5   \n",
       "3         40.778766         -73.951010         10.0              6.0      0.5   \n",
       "4         40.801169         -73.937346         12.1              3.5      0.5   \n",
       "\n",
       "   tolls_amount  total_money  tip_amount  \n",
       "0           0.0         22.8        4.05  \n",
       "1           0.0         14.7        2.94  \n",
       "2           0.0         16.1        0.00  \n",
       "3           0.0         16.5        3.30  \n",
       "4           0.0         16.1        0.00  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21504 entries, 0 to 21503\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   tpep_pickup_datetime  21504 non-null  datetime64[us]\n",
      " 1   trip_distance         21504 non-null  float64       \n",
      " 2   latitude_pickup       21504 non-null  float64       \n",
      " 3   longitude_pickup      21504 non-null  float64       \n",
      " 4   latitude_dropoff      21504 non-null  float64       \n",
      " 5   longitude_dropoff     21504 non-null  float64       \n",
      " 6   fare_amount           21504 non-null  float64       \n",
      " 7   total_surcharge       21504 non-null  float64       \n",
      " 8   mta_tax               21504 non-null  float64       \n",
      " 9   tolls_amount          21504 non-null  float64       \n",
      " 10  total_money           21504 non-null  float64       \n",
      " 11  tip_amount            21504 non-null  float64       \n",
      "dtypes: datetime64[us](1), float64(11)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>latitude_pickup</th>\n",
       "      <th>longitude_pickup</th>\n",
       "      <th>latitude_dropoff</th>\n",
       "      <th>longitude_dropoff</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>total_surcharge</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>total_money</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21504</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-01 21:59:40.056175</td>\n",
       "      <td>3.269402</td>\n",
       "      <td>40.753699</td>\n",
       "      <td>-73.967116</td>\n",
       "      <td>40.755572</td>\n",
       "      <td>-73.970761</td>\n",
       "      <td>15.170968</td>\n",
       "      <td>4.022128</td>\n",
       "      <td>0.490565</td>\n",
       "      <td>0.445686</td>\n",
       "      <td>20.129347</td>\n",
       "      <td>2.731694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 01:12:27</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>40.576961</td>\n",
       "      <td>-74.174002</td>\n",
       "      <td>-171.700000</td>\n",
       "      <td>-10.250000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-15.380000</td>\n",
       "      <td>-188.080000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-28 23:29:18.500000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>40.740439</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.989845</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-01 03:11:49</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>40.758028</td>\n",
       "      <td>-73.977698</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.400000</td>\n",
       "      <td>2.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-06-30 23:37:11.500000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>40.773633</td>\n",
       "      <td>-73.961764</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.959635</td>\n",
       "      <td>17.470000</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.050000</td>\n",
       "      <td>3.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 23:36:19</td>\n",
       "      <td>55.600000</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.735554</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>124.100000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>141.540000</td>\n",
       "      <td>111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.058293</td>\n",
       "      <td>0.031836</td>\n",
       "      <td>0.044630</td>\n",
       "      <td>0.033238</td>\n",
       "      <td>0.036692</td>\n",
       "      <td>13.986222</td>\n",
       "      <td>2.043420</td>\n",
       "      <td>0.089441</td>\n",
       "      <td>1.865712</td>\n",
       "      <td>15.737372</td>\n",
       "      <td>3.279768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tpep_pickup_datetime  trip_distance  latitude_pickup  \\\n",
       "count                       21504   21504.000000     21504.000000   \n",
       "mean   2022-05-01 21:59:40.056175       3.269402        40.753699   \n",
       "min           2020-01-01 01:12:27       0.010000        40.576961   \n",
       "25%    2021-02-28 23:29:18.500000       1.100000        40.740439   \n",
       "50%           2022-05-01 03:11:49       1.800000        40.758028   \n",
       "75%    2023-06-30 23:37:11.500000       3.400000        40.773633   \n",
       "max           2024-08-31 23:36:19      55.600000        40.899528   \n",
       "std                           NaN       4.058293         0.031836   \n",
       "\n",
       "       longitude_pickup  latitude_dropoff  longitude_dropoff   fare_amount  \\\n",
       "count      21504.000000      21504.000000       21504.000000  21504.000000   \n",
       "mean         -73.967116         40.755572         -73.970761     15.170968   \n",
       "min          -74.174002         40.576961         -74.174002   -171.700000   \n",
       "25%          -73.989845         40.740337         -73.989845      7.200000   \n",
       "50%          -73.977698         40.758028         -73.977698     10.700000   \n",
       "75%          -73.961764         40.775932         -73.959635     17.470000   \n",
       "max          -73.735554         40.899528         -73.726655    124.100000   \n",
       "std            0.044630          0.033238           0.036692     13.986222   \n",
       "\n",
       "       total_surcharge       mta_tax  tolls_amount   total_money    tip_amount  \n",
       "count     21504.000000  21504.000000  21504.000000  21504.000000  21504.000000  \n",
       "mean          4.022128      0.490565      0.445686     20.129347      2.731694  \n",
       "min         -10.250000     -0.500000    -15.380000   -188.080000      0.000000  \n",
       "25%           2.800000      0.500000      0.000000     11.800000      0.000000  \n",
       "50%           3.500000      0.500000      0.000000     15.400000      2.160000  \n",
       "75%           5.300000      0.500000      0.000000     22.050000      3.460000  \n",
       "max          17.000000      0.800000     40.000000    141.540000    111.000000  \n",
       "std           2.043420      0.089441      1.865712     15.737372      3.279768  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "This section downloads the relevant parquet files from the taxi website and creates a sample according to the sampling function of the Uber data. The sample dataframe is cleaned using the `get_and_clean_uber_data` function.\n",
    "\n",
    "* The `filter_uber_and_sample_monthly` function uses similar logic as the `sample_monthly` function, but it filters Uber trips before sampling. The function first calculates the appropriate sample size with the filtered Uber data. It reads parquet files in the directory and filter Uber data each month, then creates a sample of each month and integrate the sample datasets into one dataset.\n",
    "\n",
    "* The `get_and_clean_uber_data` function takes a dataframe and returns a cleaned dataframe that:\n",
    "    * Filtered Uber rides\n",
    "    * Converted Location IDs to latitude lognitude coordinates\n",
    "    * Computed total fares for each ride\n",
    "    * Filtered rides that start and/or end within the New York bounding box\n",
    "    * Dropped columns that are irrelevant to later parts of the project\n",
    "    * Normalized column names and removed invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "40e29d34-cecb-4b05-a695-02de44a7cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data_dir = \"uber_data\"\n",
    "download_parquet(uber_urls, uber_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "45662b42-073c-4550-b5e5-b3bfa598d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uber_monthly_sample_size(directory: str) -> int:\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".parquet\")]\n",
    "\n",
    "    max_rows = 0\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(file, columns=[\"hvfhs_license_num\"])\n",
    "            filtered_df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "            max_rows = max(max_rows, filtered_df.shape[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file}\")\n",
    "            continue  \n",
    "\n",
    "    sample_size = calculate_sample_size(max_rows)\n",
    "    print(f\"Sample size for all months: {sample_size}\")\n",
    "\n",
    "    return sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "aa9f7897-fcac-44c1-9bbc-100bb5660b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uber_sample_monthly(directory: str, sample_size: int) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Samples a specified number of rows from each Parquet file in a directory filtering Uber data only.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing Parquet files.\n",
    "        sample_size (int): Number of rows to sample from each file.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of sampled DataFrames for each month.\n",
    "    \"\"\"\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".parquet\")]\n",
    "    sampled_dataframes = []\n",
    "    columns_to_load = [\"hvfhs_license_num\", \"pickup_datetime\", \"dropoff_datetime\", \n",
    "               \"PULocationID\", \"DOLocationID\", \"trip_miles\",\n",
    "               \"base_passenger_fare\", \"bcf\", \"congestion_surcharge\", \"airport_fee\", \"sales_tax\", \"tolls\", \"tips\", \n",
    "    ]\n",
    "    \n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file, columns=columns_to_load)\n",
    "        filtered_df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "        sampled_df = filtered_df.sample(n=sample_size, random_state=30, replace=False)\n",
    "        sampled_dataframes.append(sampled_df)\n",
    "\n",
    "    print(\"Finished sampling\")\n",
    "    return sampled_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "67d4d81b-0194-49f9-8030-f2a5c1050c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for all months: 384\n",
      "Finished sampling\n"
     ]
    }
   ],
   "source": [
    "uber_sample_size = uber_monthly_sample_size(uber_data_dir)\n",
    "uber_monthly_sampled = uber_sample_monthly(uber_data_dir, uber_sample_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ffb9898b-c1d5-420e-954f-bdf0a9e6fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(dataframes: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans a list of Uber DataFrames, resamples them based on uber_sample_size,\n",
    "    and concatenates them into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        dataframes (List[pd.DataFrame]): List of Uber DataFrames to be cleaned and sampled.\n",
    "        taxi_sample_size (int): Number of samples to keep for each DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and concatenated DataFrame of sampled Uber data.\n",
    "    \"\"\"\n",
    "    \n",
    "    surcharge_columns = [\"bcf\", \"congestion_surcharge\", \"airport_fee\"]\n",
    "    fare_columns = [\"base_passenger_fare\", \"sales_tax\", \"tolls\"] + surcharge_columns\n",
    "    columns_to_keep = fare_columns + [\"pickup_lat\", \"pickup_lon\", \"dropoff_lat\", \"dropoff_lon\"]\n",
    "    try:\n",
    "        print(f\"Cleaning and sampling the list of dataframes...\")\n",
    "\n",
    "        cleaned_sampled_dataframes = []\n",
    "        for df in dataframes:\n",
    "            # convert LocationID to coordinates\n",
    "            df[[\"pickup_lat\", \"pickup_lon\"]] = df[\"PULocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "            df[[\"dropoff_lat\", \"dropoff_lon\"]] = df[\"DOLocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "            \n",
    "            # remove invalid locations & 0 mile trips\n",
    "            df = df.dropna(subset=[\"pickup_lat\", \"pickup_lon\", \"dropoff_lat\", \"dropoff_lon\"])\n",
    "            df = df[df[\"trip_miles\"] != 0]\n",
    "            \n",
    "            # filter trips within the bounding box\n",
    "            ((min_lat, min_lon), (max_lat, max_lon)) = NEW_YORK_BOX_COORDS\n",
    "            pickup_in_box = (\n",
    "                (df[\"pickup_lat\"] >= min_lat) & (df[\"pickup_lat\"] <= max_lat) &\n",
    "                (df[\"pickup_lon\"] >= min_lon) & (df[\"pickup_lon\"] <= max_lon)\n",
    "            )\n",
    "            dropoff_in_box = (\n",
    "                (df[\"dropoff_lat\"] >= min_lat) & (df[\"dropoff_lat\"] <= max_lat) &\n",
    "                (df[\"dropoff_lon\"] >= min_lon) & (df[\"dropoff_lon\"] <= max_lon)\n",
    "            )\n",
    "            df = df[pickup_in_box & dropoff_in_box]\n",
    "            \n",
    "            \n",
    "        \n",
    "            # Resample the cleaned DataFrame\n",
    "            sampled_df = df.sample(n=uber_sample_size, random_state=30, replace=False)\n",
    "            cleaned_sampled_dataframes.append(sampled_df)\n",
    "        \n",
    "        # Combine resampled DataFrames\n",
    "        if cleaned_sampled_dataframes:\n",
    "            columns_to_keep = [\"pickup_datetime\", \n",
    "                               \"pickup_lat\", \"pickup_lon\", \"dropoff_lat\", \"dropoff_lon\",\n",
    "                               \"trip_miles\", \n",
    "                               \"base_passenger_fare\", \"total_surcharge\", \"sales_tax\", \"tolls\", \"total_fare\",\n",
    "                               \"tips\"]\n",
    "            final_df = pd.concat(cleaned_sampled_dataframes, ignore_index=True)\n",
    "            \n",
    "            # compute fares\n",
    "            final_df[surcharge_columns] = final_df[surcharge_columns].fillna(0)\n",
    "            final_df[\"total_surcharge\"] = final_df[surcharge_columns].sum(axis=1)\n",
    "            final_df[fare_columns] = final_df[fare_columns].fillna(0)\n",
    "            final_df[\"total_fare\"] = final_df[fare_columns].sum(axis=1)\n",
    "            final_df = final_df[columns_to_keep]\n",
    "            return final_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the dataframes: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b8bf7dbd-49a1-49c1-b6d4-1b6767855f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and sampling the list of dataframes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/llw7py457rl4zlvsw5ymljw80000gn/T/ipykernel_54357/2840169793.py:55: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(cleaned_sampled_dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "uber_data = get_and_clean_uber_data(uber_monthly_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7902ce9d-313f-4c3e-9cb6-0e87cb1f4256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_lat</th>\n",
       "      <th>pickup_lon</th>\n",
       "      <th>dropoff_lat</th>\n",
       "      <th>dropoff_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>total_surcharge</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>tolls</th>\n",
       "      <th>total_fare</th>\n",
       "      <th>tips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-20 23:39:58</td>\n",
       "      <td>40.877137</td>\n",
       "      <td>-73.879022</td>\n",
       "      <td>40.846783</td>\n",
       "      <td>-73.850671</td>\n",
       "      <td>3.42</td>\n",
       "      <td>14.16</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.84</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-05 13:48:03</td>\n",
       "      <td>40.823318</td>\n",
       "      <td>-73.823539</td>\n",
       "      <td>40.832101</td>\n",
       "      <td>-73.848641</td>\n",
       "      <td>2.41</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>14.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-24 20:02:36</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.990458</td>\n",
       "      <td>40.748575</td>\n",
       "      <td>-73.985156</td>\n",
       "      <td>1.09</td>\n",
       "      <td>12.77</td>\n",
       "      <td>3.15</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>17.09</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-13 10:16:55</td>\n",
       "      <td>40.718337</td>\n",
       "      <td>-73.880051</td>\n",
       "      <td>40.723995</td>\n",
       "      <td>-73.902331</td>\n",
       "      <td>2.43</td>\n",
       "      <td>9.32</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.43</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-02 07:42:37</td>\n",
       "      <td>40.757312</td>\n",
       "      <td>-73.885317</td>\n",
       "      <td>40.777427</td>\n",
       "      <td>-73.905407</td>\n",
       "      <td>2.56</td>\n",
       "      <td>10.35</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.58</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-03-29 18:47:42</td>\n",
       "      <td>40.728340</td>\n",
       "      <td>-73.997380</td>\n",
       "      <td>40.780436</td>\n",
       "      <td>-73.957012</td>\n",
       "      <td>4.71</td>\n",
       "      <td>39.29</td>\n",
       "      <td>3.93</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.00</td>\n",
       "      <td>46.71</td>\n",
       "      <td>4.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-03-02 17:55:28</td>\n",
       "      <td>40.729506</td>\n",
       "      <td>-73.949540</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>-73.992438</td>\n",
       "      <td>4.18</td>\n",
       "      <td>21.08</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.41</td>\n",
       "      <td>6.12</td>\n",
       "      <td>33.18</td>\n",
       "      <td>4.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-03-14 22:52:33</td>\n",
       "      <td>40.703546</td>\n",
       "      <td>-73.875737</td>\n",
       "      <td>40.718337</td>\n",
       "      <td>-73.880051</td>\n",
       "      <td>1.54</td>\n",
       "      <td>8.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.32</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-03-11 13:13:07</td>\n",
       "      <td>40.614591</td>\n",
       "      <td>-73.915277</td>\n",
       "      <td>40.637900</td>\n",
       "      <td>-73.960968</td>\n",
       "      <td>4.06</td>\n",
       "      <td>30.42</td>\n",
       "      <td>0.91</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>34.03</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-03-28 10:52:50</td>\n",
       "      <td>40.849058</td>\n",
       "      <td>-73.905122</td>\n",
       "      <td>40.854405</td>\n",
       "      <td>-73.854394</td>\n",
       "      <td>3.44</td>\n",
       "      <td>14.94</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.72</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pickup_datetime  pickup_lat  pickup_lon  dropoff_lat  dropoff_lon  \\\n",
       "0 2021-03-20 23:39:58   40.877137  -73.879022    40.846783   -73.850671   \n",
       "1 2021-03-05 13:48:03   40.823318  -73.823539    40.832101   -73.848641   \n",
       "2 2021-03-24 20:02:36   40.740337  -73.990458    40.748575   -73.985156   \n",
       "3 2021-03-13 10:16:55   40.718337  -73.880051    40.723995   -73.902331   \n",
       "4 2021-03-02 07:42:37   40.757312  -73.885317    40.777427   -73.905407   \n",
       "5 2021-03-29 18:47:42   40.728340  -73.997380    40.780436   -73.957012   \n",
       "6 2021-03-02 17:55:28   40.729506  -73.949540    40.748497   -73.992438   \n",
       "7 2021-03-14 22:52:33   40.703546  -73.875737    40.718337   -73.880051   \n",
       "8 2021-03-11 13:13:07   40.614591  -73.915277    40.637900   -73.960968   \n",
       "9 2021-03-28 10:52:50   40.849058  -73.905122    40.854405   -73.854394   \n",
       "\n",
       "   trip_miles  base_passenger_fare  total_surcharge  sales_tax  tolls  \\\n",
       "0        3.42                14.16             0.42       1.26   0.00   \n",
       "1        2.41                13.30             0.40       1.18   0.00   \n",
       "2        1.09                12.77             3.15       1.17   0.00   \n",
       "3        2.43                 9.32             0.28       0.83   0.00   \n",
       "4        2.56                10.35             0.31       0.92   0.00   \n",
       "5        4.71                39.29             3.93       3.49   0.00   \n",
       "6        4.18                21.08             3.57       2.41   6.12   \n",
       "7        1.54                 8.33             0.25       0.74   0.00   \n",
       "8        4.06                30.42             0.91       2.70   0.00   \n",
       "9        3.44                14.94             0.45       1.33   0.00   \n",
       "\n",
       "   total_fare  tips  \n",
       "0       15.84  0.00  \n",
       "1       14.88  0.00  \n",
       "2       17.09  1.00  \n",
       "3       10.43  0.00  \n",
       "4       11.58  0.00  \n",
       "5       46.71  4.67  \n",
       "6       33.18  4.97  \n",
       "7        9.32  0.00  \n",
       "8       34.03  0.00  \n",
       "9       16.72  0.00  "
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8ac89430-73c0-42cf-b690-d9c821e2184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21504 entries, 0 to 21503\n",
      "Data columns (total 12 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   pickup_datetime      21504 non-null  datetime64[us]\n",
      " 1   pickup_lat           21504 non-null  float64       \n",
      " 2   pickup_lon           21504 non-null  float64       \n",
      " 3   dropoff_lat          21504 non-null  float64       \n",
      " 4   dropoff_lon          21504 non-null  float64       \n",
      " 5   trip_miles           21504 non-null  float64       \n",
      " 6   base_passenger_fare  21504 non-null  float64       \n",
      " 7   total_surcharge      21504 non-null  float64       \n",
      " 8   sales_tax            21504 non-null  float64       \n",
      " 9   tolls                21504 non-null  float64       \n",
      " 10  total_fare           21504 non-null  float64       \n",
      " 11  tips                 21504 non-null  float64       \n",
      "dtypes: datetime64[us](1), float64(11)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_lat</th>\n",
       "      <th>pickup_lon</th>\n",
       "      <th>dropoff_lat</th>\n",
       "      <th>dropoff_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>base_passenger_fare</th>\n",
       "      <th>total_surcharge</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>tolls</th>\n",
       "      <th>total_fare</th>\n",
       "      <th>tips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21504</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>21504.000000</td>\n",
       "      <td>2.150400e+04</td>\n",
       "      <td>21504.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-02 01:20:32.941731</td>\n",
       "      <td>40.738377</td>\n",
       "      <td>-73.933782</td>\n",
       "      <td>40.737939</td>\n",
       "      <td>-73.933268</td>\n",
       "      <td>4.431483</td>\n",
       "      <td>21.216328</td>\n",
       "      <td>1.804562</td>\n",
       "      <td>1.907293</td>\n",
       "      <td>0.668083</td>\n",
       "      <td>2.559626e+01</td>\n",
       "      <td>0.788692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:10:59</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.186421</td>\n",
       "      <td>40.561994</td>\n",
       "      <td>-74.186421</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>-15.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.332268e-15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-02-28 23:53:05</td>\n",
       "      <td>40.691507</td>\n",
       "      <td>-73.984052</td>\n",
       "      <td>40.690787</td>\n",
       "      <td>-73.983025</td>\n",
       "      <td>1.540000</td>\n",
       "      <td>10.570000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.254000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-01 00:57:50</td>\n",
       "      <td>40.739495</td>\n",
       "      <td>-73.947442</td>\n",
       "      <td>40.737698</td>\n",
       "      <td>-73.946510</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>16.610000</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.980500e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-07-01 00:10:12.500000</td>\n",
       "      <td>40.775965</td>\n",
       "      <td>-73.898957</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.895364</td>\n",
       "      <td>5.630000</td>\n",
       "      <td>26.520000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>2.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.187250e+01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-08-31 23:41:13</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>40.899528</td>\n",
       "      <td>-73.726655</td>\n",
       "      <td>61.240000</td>\n",
       "      <td>249.520000</td>\n",
       "      <td>14.240000</td>\n",
       "      <td>26.580000</td>\n",
       "      <td>47.480000</td>\n",
       "      <td>3.378200e+02</td>\n",
       "      <td>41.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.068720</td>\n",
       "      <td>0.064695</td>\n",
       "      <td>0.068818</td>\n",
       "      <td>0.067466</td>\n",
       "      <td>4.379942</td>\n",
       "      <td>15.864395</td>\n",
       "      <td>1.738572</td>\n",
       "      <td>1.488522</td>\n",
       "      <td>2.529564</td>\n",
       "      <td>1.969785e+01</td>\n",
       "      <td>2.405212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pickup_datetime    pickup_lat    pickup_lon   dropoff_lat  \\\n",
       "count                       21504  21504.000000  21504.000000  21504.000000   \n",
       "mean   2022-05-02 01:20:32.941731     40.738377    -73.933782     40.737939   \n",
       "min           2020-01-01 00:10:59     40.561994    -74.186421     40.561994   \n",
       "25%           2021-02-28 23:53:05     40.691507    -73.984052     40.690787   \n",
       "50%           2022-05-01 00:57:50     40.739495    -73.947442     40.737698   \n",
       "75%    2023-07-01 00:10:12.500000     40.775965    -73.898957     40.775932   \n",
       "max           2024-08-31 23:41:13     40.899528    -73.726655     40.899528   \n",
       "std                           NaN      0.068720      0.064695      0.068818   \n",
       "\n",
       "        dropoff_lon    trip_miles  base_passenger_fare  total_surcharge  \\\n",
       "count  21504.000000  21504.000000         21504.000000     21504.000000   \n",
       "mean     -73.933268      4.431483            21.216328         1.804562   \n",
       "min      -74.186421      0.020000           -15.950000         0.000000   \n",
       "25%      -73.983025      1.540000            10.570000         0.340000   \n",
       "50%      -73.946510      2.820000            16.610000         0.740000   \n",
       "75%      -73.895364      5.630000            26.520000         3.300000   \n",
       "max      -73.726655     61.240000           249.520000        14.240000   \n",
       "std        0.067466      4.379942            15.864395         1.738572   \n",
       "\n",
       "          sales_tax         tolls    total_fare          tips  \n",
       "count  21504.000000  21504.000000  2.150400e+04  21504.000000  \n",
       "mean       1.907293      0.668083  2.559626e+01      0.788692  \n",
       "min        0.000000      0.000000 -1.332268e-15      0.000000  \n",
       "25%        0.920000      0.000000  1.254000e+01      0.000000  \n",
       "50%        1.470000      0.000000  1.980500e+01      0.000000  \n",
       "75%        2.380000      0.000000  3.187250e+01      0.000000  \n",
       "max       26.580000     47.480000  3.378200e+02     41.360000  \n",
       "std        1.488522      2.529564  1.969785e+01      2.405212  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "In this section, we processed the weather data and creates dataframes with hourly and daily granularity information retaining relevant information only.\n",
    "*  `get_all_weather_csvs` returns the weather csv files in the directory\n",
    "*  `clean_month_weather_data_hourly` takes csv files and returns a dataframe that contains hourly precipation and wind speed information\n",
    "*  `clean_month_weather_data_daily` takes csv files and returns a dataframe that contains daily precipation, wind speed, and snowfall information. The function fills in values according to the data description for better data processing later\n",
    "*  `load_and_clean_weather_data` concatnates all daily dataframes and all hourly dataframes into two large dataframes that contains all daily weather data and all hourly data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    weather_csvs = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "    return weather_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    \n",
    "    df[\"date\"] = pd.to_datetime(df[\"DATE\"])\n",
    "    df[\"HourlyPrecipitation\"] = (df[\"HourlyPrecipitation\"]\n",
    "        .replace(\"T\", \"0.005\")  # Replace 'T' (trace) with a small float\n",
    "        .str.extract(r\"([\\d\\.]+)\")  # Extract numeric part, ignore non-numeric\n",
    "        .astype(float)  # Convert to float\n",
    "    )\n",
    "\n",
    "    columns = [\"date\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]\n",
    "    df = df[columns]\n",
    "    \n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "    df.fillna(0, inplace=True)\n",
    "    rename_map = {\n",
    "        \"HourlyPrecipitation\": \"hourly_precipitation\",\n",
    "        \"HourlyWindSpeed\": \"hourly_wind_speed\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"DATE\"])\n",
    "\n",
    "    df[\"DailyPrecipitation\"] = (df[\"DailyPrecipitation\"]\n",
    "            .replace(\"T\", \"0.005\")  \n",
    "            .str.extract(r\"([\\d\\.]+)\")  \n",
    "            .astype(float)  \n",
    "    )\n",
    "    df[\"DailySnowfall\"] = (df[\"DailySnowfall\"]\n",
    "            .replace(\"T\", \"0.005\")  \n",
    "            .str.extract(r\"([\\d\\.]+)\")  \n",
    "            .astype(float) \n",
    "    )\n",
    "    \n",
    "    columns = [\"date\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"DailySnowfall\"]\n",
    "    df = df[columns]\n",
    "    df = df.dropna(subset=columns)\n",
    "    \n",
    "    # numeric_columns = [\"DailyPrecipitation\", \"DailyAverageWindSpeed\", \"DailySnowfall\"]\n",
    "    # df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "    \n",
    "    rename_map = {\n",
    "        \"DailyPrecipitation\": \"daily_precipitation\",\n",
    "        \"DailyAverageWindSpeed\": \"daily_average_wind_speed\",\n",
    "        \"DailySnowfall\": \"daily_snowfall\",\n",
    "    }\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "f7cd53a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c8230fe2-aee4-426e-a197-142c90347b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourly_precipitation</th>\n",
       "      <th>hourly_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  hourly_precipitation  hourly_wind_speed\n",
       "0 2020-01-01 00:51:00                   0.0                8.0\n",
       "1 2020-01-01 01:51:00                   0.0                8.0\n",
       "2 2020-01-01 02:51:00                   0.0               14.0\n",
       "3 2020-01-01 03:51:00                   0.0               11.0\n",
       "4 2020-01-01 04:51:00                   0.0                6.0"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 56098 entries, 0 to 11638\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   date                  56098 non-null  datetime64[ns]\n",
      " 1   hourly_precipitation  56098 non-null  float64       \n",
      " 2   hourly_wind_speed     56098 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourly_precipitation</th>\n",
       "      <th>hourly_wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56098</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-29 21:14:19.618881024</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>4.537238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-18 19:01:45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-28 01:21:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-15 05:39:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>2237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>13.883208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  hourly_precipitation  hourly_wind_speed\n",
       "count                          56098          56098.000000       56098.000000\n",
       "mean   2022-05-29 21:14:19.618881024              0.010841           4.537238\n",
       "min              2020-01-01 00:51:00              0.000000           0.000000\n",
       "25%              2021-03-18 19:01:45              0.000000           0.000000\n",
       "50%              2022-05-28 01:21:00              0.000000           5.000000\n",
       "75%              2023-08-15 05:39:00              0.000000           7.000000\n",
       "max              2024-10-22 18:51:00              3.470000        2237.000000\n",
       "std                              NaN              0.056735          13.883208"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_precipitation</th>\n",
       "      <th>daily_average_wind_speed</th>\n",
       "      <th>daily_snowfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-01-01 23:59:00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-01-02 23:59:00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2020-01-03 23:59:00</td>\n",
       "      <td>0.150</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2020-01-04 23:59:00</td>\n",
       "      <td>0.270</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2020-01-05 23:59:00</td>\n",
       "      <td>0.005</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date  daily_precipitation  daily_average_wind_speed  \\\n",
       "24  2020-01-01 23:59:00                0.000                       8.6   \n",
       "49  2020-01-02 23:59:00                0.000                       5.4   \n",
       "86  2020-01-03 23:59:00                0.150                       3.4   \n",
       "144 2020-01-04 23:59:00                0.270                       4.4   \n",
       "169 2020-01-05 23:59:00                0.005                      11.3   \n",
       "\n",
       "     daily_snowfall  \n",
       "24              0.0  \n",
       "49              0.0  \n",
       "86              0.0  \n",
       "144             0.0  \n",
       "169             0.0  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1692 entries, 24 to 11637\n",
      "Data columns (total 4 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   date                      1692 non-null   datetime64[ns]\n",
      " 1   daily_precipitation       1692 non-null   float64       \n",
      " 2   daily_average_wind_speed  1692 non-null   float64       \n",
      " 3   daily_snowfall            1692 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(3)\n",
      "memory usage: 66.1 KB\n"
     ]
    }
   ],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>daily_precipitation</th>\n",
       "      <th>daily_average_wind_speed</th>\n",
       "      <th>daily_snowfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1692</td>\n",
       "      <td>1692.000000</td>\n",
       "      <td>1692.000000</td>\n",
       "      <td>1692.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-06-11 16:53:28.085106176</td>\n",
       "      <td>0.145018</td>\n",
       "      <td>5.000355</td>\n",
       "      <td>0.040721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 23:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-04-05 17:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-06-19 11:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-16 05:59:00</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>6.325000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-21 23:59:00</td>\n",
       "      <td>7.130000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420440</td>\n",
       "      <td>2.339679</td>\n",
       "      <td>0.502493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  daily_precipitation  \\\n",
       "count                           1692          1692.000000   \n",
       "mean   2022-06-11 16:53:28.085106176             0.145018   \n",
       "min              2020-01-01 23:59:00             0.000000   \n",
       "25%              2021-04-05 17:59:00             0.000000   \n",
       "50%              2022-06-19 11:59:00             0.000000   \n",
       "75%              2023-08-16 05:59:00             0.060000   \n",
       "max              2024-10-21 23:59:00             7.130000   \n",
       "std                              NaN             0.420440   \n",
       "\n",
       "       daily_average_wind_speed  daily_snowfall  \n",
       "count               1692.000000     1692.000000  \n",
       "mean                   5.000355        0.040721  \n",
       "min                    0.600000        0.000000  \n",
       "25%                    3.200000        0.000000  \n",
       "50%                    4.600000        0.000000  \n",
       "75%                    6.325000        0.000000  \n",
       "max                   14.200000       14.800000  \n",
       "std                    2.339679        0.502493  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATETIME,\n",
    "    daily_precipitation FLOAT,\n",
    "    daily_average_wind_speed FLOAT,\n",
    "    daily_snowfall FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips(\n",
    "     tpep_pickup_datetime DATETIME, \n",
    "     trip_distance FLOAT, \n",
    "     latitude_pickup FLOAT, \n",
    "     longitude_pickup FLOAT, \n",
    "     latitude_dropoff FLOAT, \n",
    "     longitude_dropoff FLOAT, \n",
    "     fare_amount FLOAT, \n",
    "     total_surcharge FLOAT, \n",
    "     mta_tax FLOAT, \n",
    "     tolls_amount FLOAT, \n",
    "     total_money FLOAT,\n",
    "     tip_amount FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips(\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_lat FLOAT,\n",
    "    pickup_lon FLOAT,\n",
    "    dropoff_lat FLOAT,\n",
    "    dropoff_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    base_passenger_fare FLOAT,\n",
    "    total_surcharge FLOAT,\n",
    "    sales_tax FLOAT,\n",
    "    tolls FLOAT,\n",
    "    total_fare FLOAT,\n",
    "    tips FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "02eccdba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    with open(DATABASE_SCHEMA_FILE, \"r\") as f:\n",
    "        schema_sql = f.read()\n",
    "    schema_stmts = [stmt.strip() for stmt in schema_sql.split(\";\") if stmt.strip()]\n",
    "    for stmt in schema_stmts:\n",
    "        connection.execute(db.text(stmt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    with engine.connect() as connection:\n",
    "        for table_name, df in table_to_df_dict.items():\n",
    "            \n",
    "            df.to_sql(table_name, con=connection, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    query_filepath = os.path.join(QUERY_DIRECTORY, outfile)\n",
    "    with open(query_filepath, 'w') as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"taxi_most_popular_hour.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%H', tpep_pickup_datetime) AS hour,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM \n",
    "    taxi_trips\n",
    "WHERE \n",
    "    tpep_pickup_datetime BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "GROUP BY \n",
    "    hour\n",
    "ORDER BY \n",
    "    ride_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "df_results1 = pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5327fccd-4856-417b-b1dc-8d894f293231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hour  ride_count\n",
      "0    17        1501\n",
      "1    18        1455\n",
      "2    15        1450\n",
      "3    16        1352\n",
      "4    19        1316\n",
      "5    13        1271\n",
      "6    14        1269\n",
      "7    12        1229\n",
      "8    11        1150\n",
      "9    20        1079\n",
      "10   10        1074\n",
      "11   21         987\n",
      "12   09         908\n",
      "13   22         891\n",
      "14   08         785\n",
      "15   23         751\n",
      "16   07         606\n",
      "17   00         506\n",
      "18   06         350\n",
      "19   01         326\n",
      "20   02         212\n",
      "21   03         148\n",
      "22   05         128\n",
      "23   04         112\n"
     ]
    }
   ],
   "source": [
    "print(df_results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05591da3-068d-421f-b1e1-4bd53e493406",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "647cc27e-3626-4be8-b354-be7be0094bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"uber_most_popular_day.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,  -- 0 = Sunday, 6 = Saturday\n",
    "    COUNT(*) AS ride_count\n",
    "FROM \n",
    "    uber_trips\n",
    "WHERE \n",
    "    pickup_datetime BETWEEN '2020-01-01' AND '2024-08-31'\n",
    "GROUP BY \n",
    "    day_of_week\n",
    "ORDER BY \n",
    "    ride_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1c919940-cf72-4a31-b5f6-42a101a555b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_2)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "df_results2 = pd.read_sql(QUERY_2, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "d61c78aa-be9b-4679-8695-887882fedc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4cce2a7-9629-475e-a603-dbf816a274c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  day_of_week  ride_count\n",
      "0           6        3372\n",
      "1           5        3287\n",
      "2           4        3009\n",
      "3           0        2926\n",
      "4           3        2915\n",
      "5           2        2697\n",
      "6           1        2425\n"
     ]
    }
   ],
   "source": [
    "print(df_results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26aa12-e761-48b8-9783-5815eb6b0989",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b6785837-9ff6-4cdc-b70d-8b65d8edf97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = \"jan_per_distance.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "SELECT \n",
    "    trip_distance\n",
    "FROM (\n",
    "    SELECT \n",
    "        CAST(trip_distance AS FLOAT) AS trip_distance\n",
    "    FROM taxi_trips \n",
    "    WHERE tpep_pickup_datetime BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        CAST(trip_miles AS FLOAT) AS trip_distance\n",
    "    FROM uber_trips \n",
    "    WHERE pickup_datetime BETWEEN '2024-01-01' AND '2024-01-31'\n",
    ") AS combined_results\n",
    "WHERE trip_distance IS NOT NULL\n",
    "ORDER BY trip_distance\n",
    "LIMIT 1 OFFSET (\n",
    "    SELECT CAST(COUNT(*) * 0.95 AS INTEGER) \n",
    "    FROM (\n",
    "        SELECT \n",
    "            CAST(trip_distance AS FLOAT) AS trip_distance\n",
    "        FROM taxi_trips \n",
    "        WHERE tpep_pickup_datetime BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            CAST(trip_miles AS FLOAT) AS trip_distance\n",
    "        FROM uber_trips \n",
    "        WHERE pickup_datetime BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "    )\n",
    ") - 1;\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b60b5c07-e5ab-45cc-9e0b-72b6ee00bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_3)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "df_results3 = pd.read_sql(QUERY_3, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bb16adca-d005-4ccb-a4c5-9701caa16962",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "97200025-a70d-4505-a506-358ded444540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   trip_distance\n",
      "0          13.45\n"
     ]
    }
   ],
   "source": [
    "print(df_results3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14a5e8-4bb3-4cb4-94c3-d48b5f62f7d3",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d2f2e906-2600-4219-9d72-dfce6b1ec006",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"top10_busiest_day.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH combined_rides AS (\n",
    "    SELECT \n",
    "        DATE(tpep_pickup_datetime) AS trip_date,\n",
    "        COUNT(*) AS total_rides,\n",
    "        AVG(trip_distance) AS avg_distance\n",
    "    FROM taxi_trips\n",
    "    WHERE tpep_pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    GROUP BY trip_date\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT \n",
    "        DATE(pickup_datetime) AS trip_date,\n",
    "        COUNT(*) AS total_rides,\n",
    "        AVG(trip_miles) AS avg_distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n",
    "    GROUP BY trip_date\n",
    "),\n",
    "daily_stats AS (\n",
    "    SELECT\n",
    "        trip_date,\n",
    "        SUM(total_rides) AS total_rides,\n",
    "        AVG(avg_distance) AS avg_distance\n",
    "    FROM combined_rides\n",
    "    GROUP BY trip_date\n",
    ")\n",
    "SELECT \n",
    "    ds.trip_date,\n",
    "    ds.total_rides,\n",
    "    ds.avg_distance,\n",
    "    dw.daily_precipitation AS avg_precipitation,\n",
    "    dw.daily_average_wind_speed AS avg_wind_speed\n",
    "FROM daily_stats ds\n",
    "LEFT JOIN daily_weather dw ON ds.trip_date = DATE(dw.date)\n",
    "ORDER BY ds.total_rides DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "97ac77ab-594a-4e38-9329-0d212363112b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such column: dw.date\n[SQL: \nWITH combined_rides AS (\n    SELECT \n        DATE(tpep_pickup_datetime) AS trip_date,\n        COUNT(*) AS total_rides,\n        AVG(trip_distance) AS avg_distance\n    FROM taxi_trips\n    WHERE tpep_pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n    GROUP BY trip_date\n\n    UNION ALL\n\n    SELECT \n        DATE(pickup_datetime) AS trip_date,\n        COUNT(*) AS total_rides,\n        AVG(trip_miles) AS avg_distance\n    FROM uber_trips\n    WHERE pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n    GROUP BY trip_date\n),\ndaily_stats AS (\n    SELECT\n        trip_date,\n        SUM(total_rides) AS total_rides,\n        AVG(avg_distance) AS avg_distance\n    FROM combined_rides\n    GROUP BY trip_date\n)\nSELECT \n    ds.trip_date,\n    ds.total_rides,\n    ds.avg_distance,\n    dw.daily_precipitation AS avg_precipitation,\n    dw.daily_average_wind_speed AS avg_wind_speed\nFROM daily_stats ds\nLEFT JOIN daily_weather dw ON ds.trip_date = DATE(dw.date)\nORDER BY ds.total_rides DESC\nLIMIT 10;\n]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[1;32m   1968\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[1;32m   1969\u001b[0m         )\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/default.py:924\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 924\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such column: dw.date",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# execute query either via sqlalchemy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m con:\n\u001b[0;32m----> 3\u001b[0m     results \u001b[38;5;241m=\u001b[39m con\u001b[38;5;241m.\u001b[39mexecute(db\u001b[38;5;241m.\u001b[39mtext(QUERY_4))\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m      4\u001b[0m results\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# or via pandas\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1418\u001b[0m, in \u001b[0;36mConnection.execute\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m meth(\n\u001b[1;32m   1419\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1420\u001b[0m         distilled_parameters,\n\u001b[1;32m   1421\u001b[0m         execution_options \u001b[38;5;129;01mor\u001b[39;00m NO_OPTIONS,\n\u001b[1;32m   1422\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/sql/elements.py:515\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[0;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\u001b[38;5;241m.\u001b[39m_execute_clauseelement(\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m, distilled_params, execution_options\n\u001b[1;32m    517\u001b[0m     )\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1640\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[0;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[1;32m   1628\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1629\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[1;32m   1630\u001b[0m )\n\u001b[1;32m   1632\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[1;32m   1633\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[1;32m   1634\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[1;32m   1639\u001b[0m )\n\u001b[0;32m-> 1640\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_context(\n\u001b[1;32m   1641\u001b[0m     dialect,\n\u001b[1;32m   1642\u001b[0m     dialect\u001b[38;5;241m.\u001b[39mexecution_ctx_cls\u001b[38;5;241m.\u001b[39m_init_compiled,\n\u001b[1;32m   1643\u001b[0m     compiled_sql,\n\u001b[1;32m   1644\u001b[0m     distilled_parameters,\n\u001b[1;32m   1645\u001b[0m     execution_options,\n\u001b[1;32m   1646\u001b[0m     compiled_sql,\n\u001b[1;32m   1647\u001b[0m     distilled_parameters,\n\u001b[1;32m   1648\u001b[0m     elem,\n\u001b[1;32m   1649\u001b[0m     extracted_params,\n\u001b[1;32m   1650\u001b[0m     cache_hit\u001b[38;5;241m=\u001b[39mcache_hit,\n\u001b[1;32m   1651\u001b[0m )\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[1;32m   1654\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1655\u001b[0m         elem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m         ret,\n\u001b[1;32m   1660\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1846\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_single_context(\n\u001b[1;32m   1847\u001b[0m         dialect, context, statement, parameters\n\u001b[1;32m   1848\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1986\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[1;32m   1985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_dbapi_exception(\n\u001b[1;32m   1987\u001b[0m         e, str_statement, effective_parameters, cursor, context\n\u001b[1;32m   1988\u001b[0m     )\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/base.py:2353\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[1;32m   2351\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[1;32m   2352\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2355\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[0;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[1;32m   1965\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[0;32m-> 1967\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mdo_execute(\n\u001b[1;32m   1968\u001b[0m             cursor, str_statement, effective_parameters, context\n\u001b[1;32m   1969\u001b[0m         )\n\u001b[1;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[1;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[1;32m   1973\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1974\u001b[0m         cursor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1978\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[1;32m   1979\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sqlalchemy/engine/default.py:924\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 924\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mexecute(statement, parameters)\n",
      "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such column: dw.date\n[SQL: \nWITH combined_rides AS (\n    SELECT \n        DATE(tpep_pickup_datetime) AS trip_date,\n        COUNT(*) AS total_rides,\n        AVG(trip_distance) AS avg_distance\n    FROM taxi_trips\n    WHERE tpep_pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n    GROUP BY trip_date\n\n    UNION ALL\n\n    SELECT \n        DATE(pickup_datetime) AS trip_date,\n        COUNT(*) AS total_rides,\n        AVG(trip_miles) AS avg_distance\n    FROM uber_trips\n    WHERE pickup_datetime BETWEEN '2023-01-01' AND '2023-12-31'\n    GROUP BY trip_date\n),\ndaily_stats AS (\n    SELECT\n        trip_date,\n        SUM(total_rides) AS total_rides,\n        AVG(avg_distance) AS avg_distance\n    FROM combined_rides\n    GROUP BY trip_date\n)\nSELECT \n    ds.trip_date,\n    ds.total_rides,\n    ds.avg_distance,\n    dw.daily_precipitation AS avg_precipitation,\n    dw.daily_average_wind_speed AS avg_wind_speed\nFROM daily_stats ds\nLEFT JOIN daily_weather dw ON ds.trip_date = DATE(dw.date)\nORDER BY ds.total_rides DESC\nLIMIT 10;\n]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_4)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "df_results4 = pd.read_sql(QUERY_4, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a91a70-c23f-4994-b630-5011c6e6e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d968de-f484-4855-b7e4-8183e05440e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760be244-dee8-4ee8-8ef1-a0b19d603882",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f1254-a234-421a-a4bf-b9b0e5dfccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2734136-3e79-4153-b72f-cb8d0701d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_5)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "df_results5 = pd.read_sql(QUERY_5, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217a483-657d-400e-b88f-74d394646b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752b461-47e6-4c57-a7ff-b42c533a4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f9a7f6-2f0a-4112-b0a7-eaa376d13efe",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfd93a4-fbe4-4d36-86cc-1b2efd8a3549",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09957503-b834-4448-8f6d-c53f60a9e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_6)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "df_results6 = pd.read_sql(QUERY_6, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8fced-fb4f-41ff-804b-6c2ab027d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af1611-289c-4639-ae97-71692e0e9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def taxi_popular_hour(dataframe):\n",
    "    # Sort the data by the 'hour' column to ensure it's in order\n",
    "    df_sorted = df.sort_values(by=\"hour\")\n",
    "    \n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(df_sorted[\"hour\"], df_sorted[\"ride_count\"], color=\"skyblue\", edgecolor=\"black\")\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Hour\", fontsize=12)\n",
    "    plt.ylabel(\"Number of Rides\", fontsize=12)\n",
    "    plt.title(\"Number of Rides by Hour of the Day\", fontsize=14)\n",
    "    plt.xticks(df_sorted[\"hour\"], fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924acd44-0916-4cd7-8d37-830b1d6aea16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
