{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "WEATHER_CSV_DIR = \"weather_data\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones\n",
    "In this section, we loaded the taxi shapefile that corresponds location IDs to geographical latitudes and logitudes. \n",
    "* The `load_taxi_zones` function reads the shapefile and use GeoPandas to read the file\n",
    "* The `lookup_coords_for_taxi_zone_id` function takes location IDs and the loaded shapefile and returns a tuple of latitude and logitude\n",
    "* The `make_loc_id_coords_dict` creates a dictionary of location IDs and coordinates that will be used in data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    taxi_zones = gpd.read_file(shapefile)\n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5296e8e3-e831-4d68-b1a2-8af4d49c949d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Projected CRS: EPSG:2263>\n",
       "Name: NAD83 / New York Long Island (ftUS)\n",
       "Axis Info [cartesian]:\n",
       "- X[east]: Easting (US survey foot)\n",
       "- Y[north]: Northing (US survey foot)\n",
       "Area of Use:\n",
       "- name: United States (USA) - New York - counties of Bronx; Kings; Nassau; New York; Queens; Richmond; Suffolk.\n",
       "- bounds: (-74.26, 40.47, -71.8, 41.3)\n",
       "Coordinate Operation:\n",
       "- name: SPCS83 New York Long Island zone (US survey foot)\n",
       "- method: Lambert Conic Conformal (2SP)\n",
       "Datum: North American Datum 1983\n",
       "- Ellipsoid: GRS 1980\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "taxi_zones.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a924269c-1348-4110-ba72-326f2f258264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones=taxi_zones):\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones[\"LocationID\"] == zone_loc_id]\n",
    "    centroid = zone.geometry.centroid.iloc[0]\n",
    "    centroid_geo = gpd.GeoSeries([centroid], crs=loaded_taxi_zones.crs).to_crs(epsg=CRS).iloc[0]\n",
    "\n",
    "    latitude = centroid_geo.y\n",
    "    longitude = centroid_geo.x\n",
    "\n",
    "    return (latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "dae65882-f709-4348-80a0-6276cd04eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loc_id_coords_dict(loaded_taxi_zones):\n",
    "    id_coords_dict = {}\n",
    "    for loc_id in loaded_taxi_zones[\"LocationID\"]:\n",
    "        id_coords_dict[loc_id] = lookup_coords_for_taxi_zone_id(loc_id, loaded_taxi_zones)\n",
    "\n",
    "    return id_coords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "90e63ccc-1ff3-46cf-8252-d402e9cf1a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ID_COORDS_DICT = make_loc_id_coords_dict(taxi_zones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population):\n",
    "    confidence_level = 0.95\n",
    "    margin_of_error = 0.05\n",
    "    proportion = 0.5\n",
    "    \n",
    "    from scipy.stats import norm\n",
    "\n",
    "    z_score = norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "\n",
    "    # Cochran’s\n",
    "    n = (z_score**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population\n",
    "    n_adj = n / (1 + (n - 1) / population)\n",
    "    \n",
    "    return int(round(n_adj)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions\n",
    "* `get_all_urls_from_taxi_page` fetches information on the taxi page and finds all \"Yellow Taxi Trip Records\" urls and \"High Volume For-Hire Vehicle Trip Records\" urls\n",
    "* `find_parquet_urls` uses regex to filter the urls that ends with \".parquet\" to make sure that the urls are parquet files\n",
    "* `download_parquet` creates a directory and downloads relevant parquets into the directory\n",
    "* `get_and_clean_month` filters the urls that are from January 2020 to August 2024\n",
    "* `sample_monthly` function reads all the parquet files in a directory, finds the file with largest number of rows and computes the sample size using the \"maximum population\". Next it creates samples for all files using the computed sample size and combine them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    response = requests.get(taxi_page)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    yellow_tags = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    "    fhvhv_tags = soup.find_all(\"a\", attrs={\"title\": \"High Volume For-Hire Vehicle Trip Records\"})\n",
    "\n",
    "    yellow_urls = [a[\"href\"].strip() for a in yellow_tags]\n",
    "    fhvhv_urls = [a[\"href\"].strip() for a in fhvhv_tags]\n",
    "    \n",
    "    return yellow_urls, fhvhv_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parquet_urls(urls):\n",
    "    pattern = re.compile(r\"\\.parquet$\")\n",
    "    parquet_urls = [url for url in urls if pattern.search(url)]    \n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b77c3ef5-812f-45db-bed1-d9f8ef52269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parquet(urls, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for url in urls:\n",
    "        filename = os.path.basename(url)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        if os.path.exists(output_path):\n",
    "            continue        \n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded {filename} to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(urls):\n",
    "    pattern = re.compile(r\"(202[0-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\")\n",
    "    cleaned_urls = [url for url in urls if pattern.search(url)]\n",
    "    return cleaned_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c60ef2-3ab2-44f2-938e-05cbf8868f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_to_df(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".parquet\")]\n",
    "    all_dataframe = []\n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_parquet(file)\n",
    "        all_dataframe.append(df)\n",
    "    if all_dataframe:\n",
    "        combined_df = pd.concat(all_dataframe, ignore_index=True)\n",
    "        return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "116a75c2-0bfb-4149-9548-47a35f3f2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parquet_column(file_path, columns_to_keep):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    cleaned_df = df[columns_to_keep]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7e11d08-6d62-4acd-bf15-3e1fe5ec5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_urls, fhvhv_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "taxi_parquet = find_parquet_urls(yellow_urls)\n",
    "uber_parquet = find_parquet_urls(fhvhv_urls)\n",
    "taxi_urls = get_and_clean_month(taxi_parquet)\n",
    "uber_urls = get_and_clean_month(uber_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "c6396fb8-bea9-4db3-a918-59eeeee34518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_monthly(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    \n",
    "    max_rows = 0\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        max_rows = max(max_rows, len(df))\n",
    "\n",
    "    sample_size = calculate_sample_size(max_rows)\n",
    "    print(f\"Sample size for all months: {sample_size}\")\n",
    "\n",
    "    sampled_dataframes = []\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        sampled_df = df.sample(n=sample_size, random_state=30, replace=False)\n",
    "        sampled_dataframes.append(sampled_df)\n",
    "    print(\"Finished sampling\")\n",
    "\n",
    "    if sampled_dataframes:\n",
    "        combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n",
    "        return combined_sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "cb0a333f-84b4-40c6-86e7-f2d56ecba837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taxi_data_dir = \"taxi_data\"\n",
    "download_parquet(taxi_urls, taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "882a02c9-5fcc-45ac-bdfc-c3b6fb6f1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for all months: 384\n",
      "Finished sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/llw7py457rl4zlvsw5ymljw80000gn/T/ipykernel_32667/1458695063.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "sampled_taxi_df = sample_monthly(taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49122c36-026f-439e-97ff-82ea5515d31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "      <th>airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-05 06:50:05</td>\n",
       "      <td>2023-06-05 07:11:09</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>27.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-12 11:11:39</td>\n",
       "      <td>2023-06-12 11:20:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>239</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>12.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.10</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-07 18:27:39</td>\n",
       "      <td>2023-06-07 18:46:53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.67</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>234</td>\n",
       "      <td>231</td>\n",
       "      <td>1</td>\n",
       "      <td>19.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.72</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-29 21:50:08</td>\n",
       "      <td>2023-06-29 22:04:14</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>186</td>\n",
       "      <td>230</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.20</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-10 22:25:59</td>\n",
       "      <td>2023-06-10 22:36:26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>246</td>\n",
       "      <td>249</td>\n",
       "      <td>1</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.80</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-20 08:31:24</td>\n",
       "      <td>2023-06-20 09:22:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.40</td>\n",
       "      <td>6.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.70</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.75</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-27 08:40:33</td>\n",
       "      <td>2023-06-27 08:47:33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-20 14:08:07</td>\n",
       "      <td>2023-06-20 14:21:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>79</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>12.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.16</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-22 18:17:30</td>\n",
       "      <td>2023-06-22 18:35:05</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>48</td>\n",
       "      <td>230</td>\n",
       "      <td>2</td>\n",
       "      <td>15.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.10</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-22 14:13:38</td>\n",
       "      <td>2023-06-22 14:27:20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>144</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2023-06-05 06:50:05   2023-06-05 07:11:09              3.0   \n",
       "1         2  2023-06-12 11:11:39   2023-06-12 11:20:59              1.0   \n",
       "2         2  2023-06-07 18:27:39   2023-06-07 18:46:53              1.0   \n",
       "3         1  2023-06-29 21:50:08   2023-06-29 22:04:14              2.0   \n",
       "4         1  2023-06-10 22:25:59   2023-06-10 22:36:26              1.0   \n",
       "5         2  2023-06-20 08:31:24   2023-06-20 09:22:59              1.0   \n",
       "6         1  2023-06-27 08:40:33   2023-06-27 08:47:33              1.0   \n",
       "7         2  2023-06-20 14:08:07   2023-06-20 14:21:00              1.0   \n",
       "8         2  2023-06-22 18:17:30   2023-06-22 18:35:05              5.0   \n",
       "9         2  2023-06-22 14:13:38   2023-06-22 14:27:20              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           5.55         1.0                  N           132           197   \n",
       "1           1.71         1.0                  N           239            48   \n",
       "2           2.67         1.0                  N           234           231   \n",
       "3           1.10         1.0                  N           186           230   \n",
       "4           1.20         1.0                  N           246           249   \n",
       "5          11.09         1.0                  N           138           143   \n",
       "6           1.20         1.0                  N           236           151   \n",
       "7           1.58         1.0                  N            79           233   \n",
       "8           0.86         1.0                  N            48           230   \n",
       "9           3.99         1.0                  N           144            88   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1         27.5    0.0      0.5        3.25          0.00   \n",
       "1             2         12.1    0.0      0.5        0.00          0.00   \n",
       "2             1         19.1    2.5      0.5        5.12          0.00   \n",
       "3             1         13.5    3.5      0.5        3.70          0.00   \n",
       "4             1         10.7    3.5      0.5        3.10          0.00   \n",
       "5             1         52.0    5.0      0.5       10.40          6.55   \n",
       "6             1          7.9    2.5      0.5        2.40          0.00   \n",
       "7             1         12.8    0.0      0.5        3.36          0.00   \n",
       "8             2         15.6    2.5      0.5        0.00          0.00   \n",
       "9             1         20.5    0.0      0.5        2.00          0.00   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \\\n",
       "0                    1.0         34.00                   0.0         1.75   \n",
       "1                    1.0         16.10                   2.5         0.00   \n",
       "2                    1.0         30.72                   2.5         0.00   \n",
       "3                    1.0         22.20                   2.5         0.00   \n",
       "4                    1.0         18.80                   2.5         0.00   \n",
       "5                    1.0         79.70                   2.5         1.75   \n",
       "6                    1.0         14.30                   2.5         0.00   \n",
       "7                    1.0         20.16                   2.5         0.00   \n",
       "8                    1.0         22.10                   2.5         0.00   \n",
       "9                    1.0         26.50                   2.5         0.00   \n",
       "\n",
       "   airport_fee  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_taxi_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "731be965-3963-443f-97cf-d57d9d826bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(directory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c7ece24-fb3f-448b-9add-89fb09144864",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m taxi_df \u001b[38;5;241m=\u001b[39m get_and_clean_taxi_data(taxi_data_dir)\n",
      "Cell \u001b[0;32mIn[48], line 7\u001b[0m, in \u001b[0;36mget_and_clean_taxi_data\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[1;32m      6\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file)\n\u001b[0;32m----> 7\u001b[0m     df \u001b[38;5;241m=\u001b[39m clean_parquet_column(file_path, columns_to_keep)\n\u001b[1;32m      8\u001b[0m     all_dataframes\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_dataframes:\n",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m, in \u001b[0;36mclean_parquet_column\u001b[0;34m(file_path, columns_to_keep)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclean_parquet_column\u001b[39m(file_path, columns_to_keep):\n\u001b[0;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(file_path)\n\u001b[1;32m      3\u001b[0m     cleaned_df \u001b[38;5;241m=\u001b[39m df[columns_to_keep]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_df\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    668\u001b[0m     path,\n\u001b[1;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    676\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    268\u001b[0m     path,\n\u001b[1;32m    269\u001b[0m     filesystem,\n\u001b[1;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    277\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    278\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n\u001b[1;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/parquet/core.py:2956\u001b[0m, in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2950\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword is no longer supported with the new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2951\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets-based implementation. Specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2952\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_legacy_dataset=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to temporarily recover the old \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2953\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbehaviour.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2954\u001b[0m     )\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2956\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _ParquetDatasetV2(\n\u001b[1;32m   2957\u001b[0m         source,\n\u001b[1;32m   2958\u001b[0m         schema\u001b[38;5;241m=\u001b[39mschema,\n\u001b[1;32m   2959\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m   2960\u001b[0m         partitioning\u001b[38;5;241m=\u001b[39mpartitioning,\n\u001b[1;32m   2961\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map,\n\u001b[1;32m   2962\u001b[0m         read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[1;32m   2963\u001b[0m         buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   2964\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m   2965\u001b[0m         ignore_prefixes\u001b[38;5;241m=\u001b[39mignore_prefixes,\n\u001b[1;32m   2966\u001b[0m         pre_buffer\u001b[38;5;241m=\u001b[39mpre_buffer,\n\u001b[1;32m   2967\u001b[0m         coerce_int96_timestamp_unit\u001b[38;5;241m=\u001b[39mcoerce_int96_timestamp_unit,\n\u001b[1;32m   2968\u001b[0m         thrift_string_size_limit\u001b[38;5;241m=\u001b[39mthrift_string_size_limit,\n\u001b[1;32m   2969\u001b[0m         thrift_container_size_limit\u001b[38;5;241m=\u001b[39mthrift_container_size_limit,\n\u001b[1;32m   2970\u001b[0m     )\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m   2972\u001b[0m     \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m     \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[1;32m   2974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/parquet/core.py:2496\u001b[0m, in \u001b[0;36m_ParquetDatasetV2.__init__\u001b[0;34m(self, path_or_paths, filesystem, filters, partitioning, read_dictionary, buffer_size, memory_map, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, schema, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, **kwargs)\u001b[0m\n\u001b[1;32m   2492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2493\u001b[0m     fragment \u001b[38;5;241m=\u001b[39m parquet_format\u001b[38;5;241m.\u001b[39mmake_fragment(single_file, filesystem)\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mFileSystemDataset(\n\u001b[0;32m-> 2496\u001b[0m         [fragment], schema\u001b[38;5;241m=\u001b[39mschema \u001b[38;5;129;01mor\u001b[39;00m fragment\u001b[38;5;241m.\u001b[39mphysical_schema,\n\u001b[1;32m   2497\u001b[0m         \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mparquet_format,\n\u001b[1;32m   2498\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfragment\u001b[38;5;241m.\u001b[39mfilesystem\n\u001b[1;32m   2499\u001b[0m     )\n\u001b[1;32m   2500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2502\u001b[0m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/_dataset.pyx:1358\u001b[0m, in \u001b[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/error.pxi:154\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pyarrow/error.pxi:91\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "taxi_df = get_and_clean_taxi_data(taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7d7994-3c72-4d14-a539-834851cf8aa4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "all_parquet_urls = find_parquet_urls(all_urls)\n",
    "cleaned_month_urls = get_and_clean_taxi_month(all_parquet_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    all_parquet_urls = find_parquet_urls(all_urls)\n",
    "    cleaned_month_urls = get_and_clean_taxi_month(all_parquet_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(cleaned_month_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "This section downloads the relevant parquet files from the taxi website and creates a sample according to the sampling function of the Uber data. The sample dataframe is cleaned using the `get_and_clean_uber_data` function.\n",
    "\n",
    "* The `filter_uber_and_sample_monthly` function uses similar logic as the `sample_monthly` function, but it filters Uber trips before sampling\n",
    "\n",
    "* reads parquet files in the directory and filter Uber data each month, then creates a sample of each month and integrate the sample datasets into one dataset.\n",
    "\n",
    "* The `get_and_clean_uber_data` function takes a dataframe and returns a cleaned dataframe that:\n",
    "    * Filtered Uber rides\n",
    "    * Converted Location IDs to latitude lognitude coordinates\n",
    "    * Computed total fares for each ride\n",
    "    * Filtered rides that start and/or end within the New York bounding box\n",
    "    * Dropped columns that are irrelevant to later parts of the project\n",
    "    * Normalized column names and removed invalid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "40e29d34-cecb-4b05-a695-02de44a7cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data_dir = \"uber_data\"\n",
    "download_parquet(uber_urls, uber_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "f6d8d281-4eed-4f0b-87e0-6189beef9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_uber_and_sample_monthly(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    \n",
    "    max_rows = 0\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        filtered_df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "        max_rows = max(max_rows, len(filtered_df))\n",
    "    sample_size = calculate_sample_size(max_rows)\n",
    "    print(f\"Sample size for all months: {sample_size}\")\n",
    "    \n",
    "    sampled_dataframes = []\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        filtered_df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "        sampled_df = filtered_df.sample(n=sample_size, random_state=30, replace=False)\n",
    "        sampled_dataframes.append(sampled_df)\n",
    "\n",
    "    if sampled_dataframes:\n",
    "        combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n",
    "        return combined_sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "322ce07f-c67c-41a2-8998-2e2421aef2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for all months: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/llw7py457rl4zlvsw5ymljw80000gn/T/ipykernel_32667/359863056.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "sampled_uber_df = filter_uber_and_sample_monthly(uber_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "cbca549f-a7ec-4b73-b534-52abd1702717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hvfhs_license_num</th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>originating_base_num</th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>...</th>\n",
       "      <th>sales_tax</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>tips</th>\n",
       "      <th>driver_pay</th>\n",
       "      <th>shared_request_flag</th>\n",
       "      <th>shared_match_flag</th>\n",
       "      <th>access_a_ride_flag</th>\n",
       "      <th>wav_request_flag</th>\n",
       "      <th>wav_match_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02765</td>\n",
       "      <td>B02765</td>\n",
       "      <td>2021-03-25 09:31:19</td>\n",
       "      <td>2021-03-25 09:32:12</td>\n",
       "      <td>2021-03-25 09:34:06</td>\n",
       "      <td>2021-03-25 09:39:19</td>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "      <td>1.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.21</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02888</td>\n",
       "      <td>B02888</td>\n",
       "      <td>2021-03-24 12:18:27</td>\n",
       "      <td>2021-03-24 12:23:05</td>\n",
       "      <td>2021-03-24 12:24:54</td>\n",
       "      <td>2021-03-24 12:53:16</td>\n",
       "      <td>70</td>\n",
       "      <td>35</td>\n",
       "      <td>11.59</td>\n",
       "      <td>...</td>\n",
       "      <td>2.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.45</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02865</td>\n",
       "      <td>B02865</td>\n",
       "      <td>2021-03-08 12:23:10</td>\n",
       "      <td>2021-03-08 12:25:52</td>\n",
       "      <td>2021-03-08 12:25:52</td>\n",
       "      <td>2021-03-08 12:32:20</td>\n",
       "      <td>262</td>\n",
       "      <td>229</td>\n",
       "      <td>2.19</td>\n",
       "      <td>...</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.16</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02875</td>\n",
       "      <td>B02875</td>\n",
       "      <td>2021-03-04 22:14:05</td>\n",
       "      <td>2021-03-04 22:19:17</td>\n",
       "      <td>2021-03-04 22:19:38</td>\n",
       "      <td>2021-03-04 22:25:14</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>0.83</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.10</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HV0003</td>\n",
       "      <td>B02395</td>\n",
       "      <td>B02395</td>\n",
       "      <td>2021-03-16 05:03:32</td>\n",
       "      <td>2021-03-16 05:10:15</td>\n",
       "      <td>2021-03-16 05:12:15</td>\n",
       "      <td>2021-03-16 05:26:09</td>\n",
       "      <td>60</td>\n",
       "      <td>168</td>\n",
       "      <td>2.71</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.94</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  hvfhs_license_num dispatching_base_num originating_base_num  \\\n",
       "0            HV0003               B02765               B02765   \n",
       "1            HV0003               B02888               B02888   \n",
       "2            HV0003               B02865               B02865   \n",
       "3            HV0003               B02875               B02875   \n",
       "4            HV0003               B02395               B02395   \n",
       "\n",
       "     request_datetime   on_scene_datetime     pickup_datetime  \\\n",
       "0 2021-03-25 09:31:19 2021-03-25 09:32:12 2021-03-25 09:34:06   \n",
       "1 2021-03-24 12:18:27 2021-03-24 12:23:05 2021-03-24 12:24:54   \n",
       "2 2021-03-08 12:23:10 2021-03-08 12:25:52 2021-03-08 12:25:52   \n",
       "3 2021-03-04 22:14:05 2021-03-04 22:19:17 2021-03-04 22:19:38   \n",
       "4 2021-03-16 05:03:32 2021-03-16 05:10:15 2021-03-16 05:12:15   \n",
       "\n",
       "     dropoff_datetime  PULocationID  DOLocationID  trip_miles  ...  sales_tax  \\\n",
       "0 2021-03-25 09:39:19           165           165        1.05  ...       0.70   \n",
       "1 2021-03-24 12:53:16            70            35       11.59  ...       2.74   \n",
       "2 2021-03-08 12:32:20           262           229        2.19  ...       1.24   \n",
       "3 2021-03-04 22:25:14            25            25        0.83  ...       0.67   \n",
       "4 2021-03-16 05:26:09            60           168        2.71  ...       0.00   \n",
       "\n",
       "   congestion_surcharge  airport_fee  tips  driver_pay  shared_request_flag  \\\n",
       "0                  0.00          NaN   0.0        6.21                    N   \n",
       "1                  0.00          NaN   0.0       29.45                    N   \n",
       "2                  2.75          NaN   0.0        7.16                    N   \n",
       "3                  0.00          NaN   3.0        9.10                    N   \n",
       "4                  0.00          NaN   0.0       12.94                    N   \n",
       "\n",
       "   shared_match_flag  access_a_ride_flag  wav_request_flag wav_match_flag  \n",
       "0                  N                                     N              N  \n",
       "1                  N                                     N              N  \n",
       "2                  N                                     N              N  \n",
       "3                  N                                     N              N  \n",
       "4                  N                                     N              N  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_uber_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dca0d9f7-3607-4feb-ac93-810c7b3770ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21504 entries, 0 to 21503\n",
      "Data columns (total 24 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   hvfhs_license_num     21504 non-null  object        \n",
      " 1   dispatching_base_num  21504 non-null  object        \n",
      " 2   originating_base_num  15516 non-null  object        \n",
      " 3   request_datetime      21504 non-null  datetime64[us]\n",
      " 4   on_scene_datetime     15519 non-null  datetime64[us]\n",
      " 5   pickup_datetime       21504 non-null  datetime64[us]\n",
      " 6   dropoff_datetime      21504 non-null  datetime64[us]\n",
      " 7   PULocationID          21504 non-null  int64         \n",
      " 8   DOLocationID          21504 non-null  int64         \n",
      " 9   trip_miles            21504 non-null  float64       \n",
      " 10  trip_time             21504 non-null  int64         \n",
      " 11  base_passenger_fare   21504 non-null  float64       \n",
      " 12  tolls                 21504 non-null  float64       \n",
      " 13  bcf                   21504 non-null  float64       \n",
      " 14  sales_tax             21504 non-null  float64       \n",
      " 15  congestion_surcharge  21504 non-null  float64       \n",
      " 16  airport_fee           15780 non-null  float64       \n",
      " 17  tips                  21504 non-null  float64       \n",
      " 18  driver_pay            21504 non-null  float64       \n",
      " 19  shared_request_flag   21504 non-null  object        \n",
      " 20  shared_match_flag     21504 non-null  object        \n",
      " 21  access_a_ride_flag    21504 non-null  object        \n",
      " 22  wav_request_flag      21504 non-null  object        \n",
      " 23  wav_match_flag        21504 non-null  object        \n",
      "dtypes: datetime64[us](4), float64(9), int64(3), object(8)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "sampled_uber_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "ffb9898b-c1d5-420e-954f-bdf0a9e6fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(df):\n",
    "    fare_columns = [\"base_passenger_fare\", \"tolls\", \"bcf\", \"sales_tax\", \"congestion_surcharge\", \"airport_fee\"]\n",
    "    columns_to_keep = [\"request_datetime\", \"on_scene_datetime\", \"pickup_datetime\", \"dropoff_datetime\",\n",
    "                       \"pickup_lat\", \"pickup_lon\", \"dropoff_lat\", \"dropoff_lon\", \"trip_miles\",\n",
    "                       \"total_fare\", \"tips\"]\n",
    "    # filter uber data\n",
    "    df = df[df[\"hvfhs_license_num\"] == \"HV0003\"]\n",
    "\n",
    "    # convert LocationID to coordinates\n",
    "    df[[\"pickup_lat\", \"pickup_lon\"]] = df[\"PULocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "    df[[\"dropoff_lat\", \"dropoff_lon\"]] = df[\"DOLocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "    \n",
    "    # remove invalid locations & 0 mile trips\n",
    "    df = df.dropna(subset=[\"pickup_lat\", \"pickup_lon\", \"dropoff_lat\", \"dropoff_lon\"])\n",
    "    df = df[df[\"trip_miles\"] != 0]\n",
    "    \n",
    "    # filter trips within the bounding box\n",
    "    ((min_lat, min_lon), (max_lat, max_lon)) = NEW_YORK_BOX_COORDS\n",
    "    pickup_in_box = (\n",
    "        (df[\"pickup_lat\"] >= min_lat) & (df[\"pickup_lat\"] <= max_lat) &\n",
    "        (df[\"pickup_lon\"] >= min_lon) & (df[\"pickup_lon\"] <= max_lon)\n",
    "    )\n",
    "    dropoff_in_box = (\n",
    "        (df[\"dropoff_lat\"] >= min_lat) & (df[\"dropoff_lat\"] <= max_lat) &\n",
    "        (df[\"dropoff_lon\"] >= min_lon) & (df[\"dropoff_lon\"] <= max_lon)\n",
    "    )\n",
    "    df = df[pickup_in_box & dropoff_in_box]\n",
    "        \n",
    "    # compute total fare\n",
    "    df[fare_columns] = df[fare_columns].fillna(0)\n",
    "    df[\"total_fare\"] = df[fare_columns].sum(axis=1)\n",
    "\n",
    "    df = df[columns_to_keep]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "b8bf7dbd-49a1-49c1-b6d4-1b6767855f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uber_data = get_and_clean_uber_data(sampled_uber_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "7902ce9d-313f-4c3e-9cb6-0e87cb1f4256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_datetime</th>\n",
       "      <th>on_scene_datetime</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>pickup_lat</th>\n",
       "      <th>pickup_lon</th>\n",
       "      <th>dropoff_lat</th>\n",
       "      <th>dropoff_lon</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>total_fare</th>\n",
       "      <th>tips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-03-25 09:31:19</td>\n",
       "      <td>2021-03-25 09:32:12</td>\n",
       "      <td>2021-03-25 09:34:06</td>\n",
       "      <td>2021-03-25 09:39:19</td>\n",
       "      <td>40.620924</td>\n",
       "      <td>-73.956824</td>\n",
       "      <td>40.620924</td>\n",
       "      <td>-73.956824</td>\n",
       "      <td>1.05</td>\n",
       "      <td>8.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-03-24 12:18:27</td>\n",
       "      <td>2021-03-24 12:23:05</td>\n",
       "      <td>2021-03-24 12:24:54</td>\n",
       "      <td>2021-03-24 12:53:16</td>\n",
       "      <td>40.763352</td>\n",
       "      <td>-73.868395</td>\n",
       "      <td>40.664003</td>\n",
       "      <td>-73.910258</td>\n",
       "      <td>11.59</td>\n",
       "      <td>34.59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-08 12:23:10</td>\n",
       "      <td>2021-03-08 12:25:52</td>\n",
       "      <td>2021-03-08 12:25:52</td>\n",
       "      <td>2021-03-08 12:32:20</td>\n",
       "      <td>40.775932</td>\n",
       "      <td>-73.946510</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>2.19</td>\n",
       "      <td>18.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-03-04 22:14:05</td>\n",
       "      <td>2021-03-04 22:19:17</td>\n",
       "      <td>2021-03-04 22:19:38</td>\n",
       "      <td>2021-03-04 22:25:14</td>\n",
       "      <td>40.685634</td>\n",
       "      <td>-73.986114</td>\n",
       "      <td>40.685634</td>\n",
       "      <td>-73.986114</td>\n",
       "      <td>0.83</td>\n",
       "      <td>8.45</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-16 05:03:32</td>\n",
       "      <td>2021-03-16 05:10:15</td>\n",
       "      <td>2021-03-16 05:12:15</td>\n",
       "      <td>2021-03-16 05:26:09</td>\n",
       "      <td>40.833990</td>\n",
       "      <td>-73.885900</td>\n",
       "      <td>40.807347</td>\n",
       "      <td>-73.916822</td>\n",
       "      <td>2.71</td>\n",
       "      <td>16.80</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-03-04 16:27:53</td>\n",
       "      <td>2021-03-04 16:30:52</td>\n",
       "      <td>2021-03-04 16:32:20</td>\n",
       "      <td>2021-03-04 16:43:20</td>\n",
       "      <td>40.841708</td>\n",
       "      <td>-73.941399</td>\n",
       "      <td>40.837827</td>\n",
       "      <td>-73.926158</td>\n",
       "      <td>2.28</td>\n",
       "      <td>10.83</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-03-18 20:56:27</td>\n",
       "      <td>2021-03-18 21:00:19</td>\n",
       "      <td>2021-03-18 21:01:06</td>\n",
       "      <td>2021-03-18 21:13:02</td>\n",
       "      <td>40.740337</td>\n",
       "      <td>-73.990458</td>\n",
       "      <td>40.756729</td>\n",
       "      <td>-73.965146</td>\n",
       "      <td>2.54</td>\n",
       "      <td>18.02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-03-13 00:35:54</td>\n",
       "      <td>2021-03-13 00:38:16</td>\n",
       "      <td>2021-03-13 00:40:16</td>\n",
       "      <td>2021-03-13 00:49:16</td>\n",
       "      <td>40.640590</td>\n",
       "      <td>-73.976199</td>\n",
       "      <td>40.641886</td>\n",
       "      <td>-74.004653</td>\n",
       "      <td>1.39</td>\n",
       "      <td>6.29</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-03-17 15:38:12</td>\n",
       "      <td>2021-03-17 15:41:44</td>\n",
       "      <td>2021-03-17 15:43:26</td>\n",
       "      <td>2021-03-17 16:24:06</td>\n",
       "      <td>40.620924</td>\n",
       "      <td>-73.956824</td>\n",
       "      <td>40.766238</td>\n",
       "      <td>-73.995135</td>\n",
       "      <td>12.48</td>\n",
       "      <td>61.31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-03-13 12:23:41</td>\n",
       "      <td>2021-03-13 12:26:47</td>\n",
       "      <td>2021-03-13 12:27:54</td>\n",
       "      <td>2021-03-13 12:45:38</td>\n",
       "      <td>40.612218</td>\n",
       "      <td>-73.995259</td>\n",
       "      <td>40.580922</td>\n",
       "      <td>-73.961217</td>\n",
       "      <td>4.08</td>\n",
       "      <td>20.27</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     request_datetime   on_scene_datetime     pickup_datetime  \\\n",
       "0 2021-03-25 09:31:19 2021-03-25 09:32:12 2021-03-25 09:34:06   \n",
       "1 2021-03-24 12:18:27 2021-03-24 12:23:05 2021-03-24 12:24:54   \n",
       "2 2021-03-08 12:23:10 2021-03-08 12:25:52 2021-03-08 12:25:52   \n",
       "3 2021-03-04 22:14:05 2021-03-04 22:19:17 2021-03-04 22:19:38   \n",
       "4 2021-03-16 05:03:32 2021-03-16 05:10:15 2021-03-16 05:12:15   \n",
       "5 2021-03-04 16:27:53 2021-03-04 16:30:52 2021-03-04 16:32:20   \n",
       "6 2021-03-18 20:56:27 2021-03-18 21:00:19 2021-03-18 21:01:06   \n",
       "7 2021-03-13 00:35:54 2021-03-13 00:38:16 2021-03-13 00:40:16   \n",
       "8 2021-03-17 15:38:12 2021-03-17 15:41:44 2021-03-17 15:43:26   \n",
       "9 2021-03-13 12:23:41 2021-03-13 12:26:47 2021-03-13 12:27:54   \n",
       "\n",
       "     dropoff_datetime  pickup_lat  pickup_lon  dropoff_lat  dropoff_lon  \\\n",
       "0 2021-03-25 09:39:19   40.620924  -73.956824    40.620924   -73.956824   \n",
       "1 2021-03-24 12:53:16   40.763352  -73.868395    40.664003   -73.910258   \n",
       "2 2021-03-08 12:32:20   40.775932  -73.946510    40.756729   -73.965146   \n",
       "3 2021-03-04 22:25:14   40.685634  -73.986114    40.685634   -73.986114   \n",
       "4 2021-03-16 05:26:09   40.833990  -73.885900    40.807347   -73.916822   \n",
       "5 2021-03-04 16:43:20   40.841708  -73.941399    40.837827   -73.926158   \n",
       "6 2021-03-18 21:13:02   40.740337  -73.990458    40.756729   -73.965146   \n",
       "7 2021-03-13 00:49:16   40.640590  -73.976199    40.641886   -74.004653   \n",
       "8 2021-03-17 16:24:06   40.620924  -73.956824    40.766238   -73.995135   \n",
       "9 2021-03-13 12:45:38   40.612218  -73.995259    40.580922   -73.961217   \n",
       "\n",
       "   trip_miles  total_fare  tips  \n",
       "0        1.05        8.85   0.0  \n",
       "1       11.59       34.59   0.0  \n",
       "2        2.19       18.40   0.0  \n",
       "3        0.83        8.45   3.0  \n",
       "4        2.71       16.80   0.0  \n",
       "5        2.28       10.83   0.0  \n",
       "6        2.54       18.02   0.0  \n",
       "7        1.39        6.29   1.0  \n",
       "8       12.48       61.31   0.0  \n",
       "9        4.08       20.27   3.0  "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "8ac89430-73c0-42cf-b690-d9c821e2184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20650 entries, 0 to 21503\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   request_datetime   20650 non-null  datetime64[us]\n",
      " 1   on_scene_datetime  20650 non-null  datetime64[us]\n",
      " 2   pickup_datetime    20650 non-null  datetime64[us]\n",
      " 3   dropoff_datetime   20650 non-null  datetime64[us]\n",
      " 4   pickup_lat         20650 non-null  float64       \n",
      " 5   pickup_lon         20650 non-null  float64       \n",
      " 6   dropoff_lat        20650 non-null  float64       \n",
      " 7   dropoff_lon        20650 non-null  float64       \n",
      " 8   trip_miles         20650 non-null  float64       \n",
      " 9   total_fare         20650 non-null  float64       \n",
      " 10  tips               20650 non-null  float64       \n",
      "dtypes: datetime64[us](4), float64(7)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data():\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    all_urls = get_all_urls_from_tlc_page(TLC_URL)\n",
    "    all_parquet_urls = find_parquet_urls(all_urls)\n",
    "    taxi_data = get_and_clean_uber_data(all_parquet_urls)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "In this section, we processed the weather data and creates dataframes with hourly and daily granularity information retaining relevant information only.\n",
    "*  `get_all_weather_csvs` returns the weather csv files in the directory\n",
    "*  `clean_month_weather_data_hourly` takes csv files and returns a dataframe that contains hourly precipation and wind speed information\n",
    "*  `clean_month_weather_data_daily` takes csv files and returns a dataframe that contains daily precipation, wind speed, and snowfall information. The function fills in values according to the data description for better data processing later\n",
    "*  `load_and_clean_weather_data` concatnates all daily dataframes and all hourly dataframes into two large dataframes that contains all daily weather data and all hourly data respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    weather_csvs = [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "    return weather_csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    \n",
    "    df[\"date\"] = pd.to_datetime(df[\"DATE\"])\n",
    "    df[\"HourlyPrecipitation\"] = (df[\"HourlyPrecipitation\"]\n",
    "        .replace(\"T\", \"0.005\")  # Replace 'T' (trace) with a small float\n",
    "        .str.extract(r\"([\\d\\.]+)\")  # Extract numeric part, ignore non-numeric\n",
    "        .astype(float)  # Convert to float\n",
    "    )\n",
    "\n",
    "    columns = [\"date\", \"HourlyPrecipitation\", \"HourlyWindSpeed\"]\n",
    "    df = df[columns]\n",
    "    \n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "    df.fillna(0, inplace=True)\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df = pd.read_csv(csv_file, low_memory=False)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"DATE\"])\n",
    "\n",
    "    df[\"DailyPrecipitation\"] = (df[\"DailyPrecipitation\"]\n",
    "            .replace(\"T\", \"0.005\")  \n",
    "            .str.extract(r\"([\\d\\.]+)\")  \n",
    "            .astype(float)  \n",
    "    )\n",
    "    df[\"DailySnowfall\"] = (df[\"DailySnowfall\"]\n",
    "            .replace(\"T\", \"0.005\")  \n",
    "            .str.extract(r\"([\\d\\.]+)\")  \n",
    "            .astype(float) \n",
    "    )\n",
    "    df[\"DailySnowDepth\"] = (df[\"DailySnowDepth\"].replace(\"T\", \"0.005\").astype(float))\n",
    "    \n",
    "    columns = [\"date\", \"DailyPrecipitation\", \"DailyAverageWindSpeed\",\n",
    "               \"DailySnowfall\", \"DailySnowDepth\"]\n",
    "    df = df[columns]\n",
    "\n",
    "    df = df.dropna(subset=[\"date\",\"DailyPrecipitation\", \"DailyAverageWindSpeed\",\n",
    "               \"DailySnowfall\", \"DailySnowDepth\"])\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "f7cd53a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "c8230fe2-aee4-426e-a197-142c90347b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourlyprecipitation</th>\n",
       "      <th>hourlywindspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:51:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 date  hourlyprecipitation  hourlywindspeed\n",
       "0 2020-01-01 00:51:00                  0.0              8.0\n",
       "1 2020-01-01 01:51:00                  0.0              8.0\n",
       "2 2020-01-01 02:51:00                  0.0             14.0\n",
       "3 2020-01-01 03:51:00                  0.0             11.0\n",
       "4 2020-01-01 04:51:00                  0.0              6.0"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 56098 entries, 0 to 11638\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   date                 56098 non-null  datetime64[ns]\n",
      " 1   hourlyprecipitation  56098 non-null  float64       \n",
      " 2   hourlywindspeed      56098 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(2)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>hourlyprecipitation</th>\n",
       "      <th>hourlywindspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>56098</td>\n",
       "      <td>56098.000000</td>\n",
       "      <td>56098.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-05-29 21:14:19.618881024</td>\n",
       "      <td>0.010841</td>\n",
       "      <td>4.537238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 00:51:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-03-18 19:01:45</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-05-28 01:21:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-15 05:39:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-22 18:51:00</td>\n",
       "      <td>3.470000</td>\n",
       "      <td>2237.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.056735</td>\n",
       "      <td>13.883208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  hourlyprecipitation  hourlywindspeed\n",
       "count                          56098         56098.000000     56098.000000\n",
       "mean   2022-05-29 21:14:19.618881024             0.010841         4.537238\n",
       "min              2020-01-01 00:51:00             0.000000         0.000000\n",
       "25%              2021-03-18 19:01:45             0.000000         0.000000\n",
       "50%              2022-05-28 01:21:00             0.000000         5.000000\n",
       "75%              2023-08-15 05:39:00             0.000000         7.000000\n",
       "max              2024-10-22 18:51:00             3.470000      2237.000000\n",
       "std                              NaN             0.056735        13.883208"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dailyprecipitation</th>\n",
       "      <th>dailyaveragewindspeed</th>\n",
       "      <th>dailysnowfall</th>\n",
       "      <th>dailysnowdepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11335</th>\n",
       "      <td>2022-12-22 23:59:00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11389</th>\n",
       "      <td>2022-12-23 23:59:00</td>\n",
       "      <td>1.83</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>2022-12-24 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11439</th>\n",
       "      <td>2022-12-25 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11464</th>\n",
       "      <td>2022-12-26 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11489</th>\n",
       "      <td>2022-12-27 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516</th>\n",
       "      <td>2022-12-28 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11541</th>\n",
       "      <td>2022-12-29 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11566</th>\n",
       "      <td>2022-12-30 23:59:00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11637</th>\n",
       "      <td>2022-12-31 23:59:00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date  dailyprecipitation  dailyaveragewindspeed  \\\n",
       "11335 2022-12-22 23:59:00                0.23                    8.6   \n",
       "11389 2022-12-23 23:59:00                1.83                   10.0   \n",
       "11414 2022-12-24 23:59:00                0.00                   11.3   \n",
       "11439 2022-12-25 23:59:00                0.00                    8.5   \n",
       "11464 2022-12-26 23:59:00                0.00                    7.1   \n",
       "11489 2022-12-27 23:59:00                0.00                    5.5   \n",
       "11516 2022-12-28 23:59:00                0.00                    5.1   \n",
       "11541 2022-12-29 23:59:00                0.00                    6.1   \n",
       "11566 2022-12-30 23:59:00                0.00                    2.9   \n",
       "11637 2022-12-31 23:59:00                0.28                    1.8   \n",
       "\n",
       "       dailysnowfall  dailysnowdepth  \n",
       "11335          0.000             0.0  \n",
       "11389          0.005             0.0  \n",
       "11414          0.000             0.0  \n",
       "11439          0.000             0.0  \n",
       "11464          0.000             0.0  \n",
       "11489          0.000             0.0  \n",
       "11516          0.000             0.0  \n",
       "11541          0.000             0.0  \n",
       "11566          0.000             0.0  \n",
       "11637          0.000             0.0  "
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1692 entries, 24 to 11637\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   date                   1692 non-null   datetime64[ns]\n",
      " 1   dailyprecipitation     1692 non-null   float64       \n",
      " 2   dailyaveragewindspeed  1692 non-null   float64       \n",
      " 3   dailysnowfall          1692 non-null   float64       \n",
      " 4   dailysnowdepth         1692 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4)\n",
      "memory usage: 79.3 KB\n"
     ]
    }
   ],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>dailyprecipitation</th>\n",
       "      <th>dailyaveragewindspeed</th>\n",
       "      <th>dailysnowfall</th>\n",
       "      <th>dailysnowdepth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1692</td>\n",
       "      <td>1692.000000</td>\n",
       "      <td>1692.000000</td>\n",
       "      <td>1692.000000</td>\n",
       "      <td>1692.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2022-06-11 16:53:28.085106176</td>\n",
       "      <td>0.145018</td>\n",
       "      <td>5.000355</td>\n",
       "      <td>0.040721</td>\n",
       "      <td>0.160195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-01-01 23:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2021-04-05 17:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2022-06-19 11:59:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2023-08-16 05:59:00</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>6.325000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2024-10-21 23:59:00</td>\n",
       "      <td>7.130000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.800000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420440</td>\n",
       "      <td>2.339679</td>\n",
       "      <td>0.502493</td>\n",
       "      <td>1.059881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                date  dailyprecipitation  \\\n",
       "count                           1692         1692.000000   \n",
       "mean   2022-06-11 16:53:28.085106176            0.145018   \n",
       "min              2020-01-01 23:59:00            0.000000   \n",
       "25%              2021-04-05 17:59:00            0.000000   \n",
       "50%              2022-06-19 11:59:00            0.000000   \n",
       "75%              2023-08-16 05:59:00            0.060000   \n",
       "max              2024-10-21 23:59:00            7.130000   \n",
       "std                              NaN            0.420440   \n",
       "\n",
       "       dailyaveragewindspeed  dailysnowfall  dailysnowdepth  \n",
       "count            1692.000000    1692.000000     1692.000000  \n",
       "mean                5.000355       0.040721        0.160195  \n",
       "min                 0.600000       0.000000        0.000000  \n",
       "25%                 3.200000       0.000000        0.000000  \n",
       "50%                 4.600000       0.000000        0.000000  \n",
       "75%                 6.325000       0.000000        0.000000  \n",
       "max                14.200000      14.800000       14.000000  \n",
       "std                 2.339679       0.502493        1.059881  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATETIME,\n",
    "    hourly_precipitation FLOAT,\n",
    "    hourly_wind_speed FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    date DATETIME,\n",
    "    daily_precipitation FLOAT,\n",
    "    daily_average_wind_speed FLOAT,\n",
    "    daily_snowfall FLOAT,\n",
    "    daily_snow_depth FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "TODO\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips(\n",
    "    request_datetime DATETIME,\n",
    "    on_scene_datetime DATETIME,\n",
    "    pickup_datetime DATETIME,\n",
    "    dropoff_datetime DATETIME,\n",
    "    pickup_lat FLOAT,\n",
    "    pickup_lon FLOAT,\n",
    "    dropoff_lat FLOAT,\n",
    "    dropoff_lon FLOAT,\n",
    "    trip_miles FLOAT,\n",
    "    total_fare FLOAT,\n",
    "    tips FLOAT\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "TODO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute query either via sqlalchemy\n",
    "with engine.connect() as con:\n",
    "    results = con.execute(db.text(QUERY_1)).fetchall()\n",
    "results\n",
    "\n",
    "# or via pandas\n",
    "pd.read_sql(QUERY_1, con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_1(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
