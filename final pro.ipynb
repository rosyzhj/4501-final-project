{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import sqlalchemy as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants you might need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = os.path.join(TAXI_ZONES_DIR, \"taxi_zones.shp\")\n",
    "WEATHER_CSV_DIR = \"weather_data\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1128760a-87ce-4830-b152-f6f3dc702c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 'taxi_zones' directory.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"taxi_zones\", exist_ok=True)\n",
    "print(\"Created 'taxi_zones' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945be133-7289-4b60-b95e-910259d90980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory: E:\\4501 final\n"
     ]
    }
   ],
   "source": [
    "new_directory = r\"E:\\4501 final\"\n",
    "os.chdir(new_directory)\n",
    "print(f\"New working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1492efa-7861-4feb-a5cc-f985f0917d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi zones loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    taxi_zones = gpd.read_file(TAXI_ZONES_SHAPEFILE)\n",
    "    print(\"Taxi zones loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading taxi zones: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    taxi_zones = gpd.read_file(shapefile)\n",
    "    return taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e0f7f21-1d32-4491-9318-2d5f66d3fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi zones loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# check \n",
    "if not os.path.exists(TAXI_ZONES_DIR):\n",
    "    print(f\"Directory '{TAXI_ZONES_DIR}' does not exist. Please create it and add the shapefile.\")\n",
    "else:\n",
    "    TAXI_ZONES_SHAPEFILE = os.path.join(TAXI_ZONES_DIR, \"taxi_zones.shp\")\n",
    "    if not os.path.exists(TAXI_ZONES_SHAPEFILE):\n",
    "        print(f\"Shapefile '{TAXI_ZONES_SHAPEFILE}' does not exist. Please ensure it's in the directory.\")\n",
    "    else:\n",
    "        taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "        print(\"Taxi zones loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "\n",
    "def lookup_coords_for_taxi_zone_id(zone_loc_id, loaded_taxi_zones=taxi_zones):\n",
    "    zone = loaded_taxi_zones[loaded_taxi_zones[\"LocationID\"] == zone_loc_id]\n",
    "    centroid = zone.geometry.centroid.iloc[0]\n",
    "    centroid_geo = gpd.GeoSeries([centroid], crs=loaded_taxi_zones.crs).to_crs(epsg=CRS).iloc[0]\n",
    "\n",
    "    latitude = centroid_geo.y\n",
    "    longitude = centroid_geo.x\n",
    "\n",
    "    return (latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae65882-f709-4348-80a0-6276cd04eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loc_id_coords_dict(loaded_taxi_zones):\n",
    "    id_coords_dict = {}\n",
    "    for loc_id in loaded_taxi_zones[\"LocationID\"]:\n",
    "        id_coords_dict[loc_id] = lookup_coords_for_taxi_zone_id(loc_id, loaded_taxi_zones)\n",
    "\n",
    "    return id_coords_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a942e338-ef11-4c19-a7ec-98d7cae09fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones = load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "ID_COORDS_DICT = make_loc_id_coords_dict(taxi_zones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(population):\n",
    "    confidence_level = 0.95\n",
    "    margin_of_error = 0.05\n",
    "    proportion = 0.5\n",
    "    \n",
    "    from scipy.stats import norm\n",
    "\n",
    "    z_score = norm.ppf(1 - (1 - confidence_level) / 2)\n",
    "\n",
    "    # Cochran’s\n",
    "    n = (z_score**2 * proportion * (1 - proportion)) / (margin_of_error**2)\n",
    "    \n",
    "    # Adjust for finite population\n",
    "    n_adj = n / (1 + (n - 1) / population)\n",
    "    \n",
    "    return int(round(n_adj)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33eed4-b2e9-4ab3-94a8-59f04e464c98",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    response = requests.get(taxi_page)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    yellow_tags = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    "    fhvhv_tags = soup.find_all(\"a\", attrs={\"title\": \"High Volume For-Hire Vehicle Trip Records\"})\n",
    "\n",
    "    yellow_urls = [a[\"href\"].strip() for a in yellow_tags]\n",
    "    fhvhv_urls = [a[\"href\"].strip() for a in fhvhv_tags]\n",
    "    \n",
    "    return yellow_urls, fhvhv_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parquet_urls(urls):\n",
    "    pattern = re.compile(r\"\\.parquet$\")\n",
    "    parquet_urls = [url for url in urls if pattern.search(url)]    \n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b77c3ef5-812f-45db-bed1-d9f8ef52269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_parquet(urls, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for url in urls:\n",
    "        filename = os.path.basename(url)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        if os.path.exists(output_path):\n",
    "            continue        \n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024): \n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Downloaded {filename} to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(urls):\n",
    "    pattern = re.compile(r\"(202[0-3]-(0[1-9]|1[0-2])|2024-(0[1-8]))\")\n",
    "    cleaned_urls = [url for url in urls if pattern.search(url)]\n",
    "    return cleaned_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2c60ef2-3ab2-44f2-938e-05cbf8868f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_to_df(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".parquet\")]\n",
    "    all_dataframe = []\n",
    "    for file in files:\n",
    "        \n",
    "        df = pd.read_parquet(file)\n",
    "        all_dataframe.append(df)\n",
    "    if all_dataframe:\n",
    "        combined_df = pd.concat(all_dataframe, ignore_index=True)\n",
    "        return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "116a75c2-0bfb-4149-9548-47a35f3f2977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_parquet_column(file_path, columns_to_keep):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    cleaned_df = df[columns_to_keep]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7e11d08-6d62-4acd-bf15-3e1fe5ec5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_urls, fhvhv_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "taxi_parquet = find_parquet_urls(yellow_urls)\n",
    "uber_parquet = find_parquet_urls(fhvhv_urls)\n",
    "taxi_urls = get_and_clean_month(taxi_parquet)\n",
    "uber_urls = get_and_clean_month(uber_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6396fb8-bea9-4db3-a918-59eeeee34518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_monthly(directory):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    \n",
    "    max_rows = 0\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        max_rows = max(max_rows, len(df))\n",
    "\n",
    "    sample_size = calculate_sample_size(max_rows)\n",
    "    print(f\"Sample size for all months: {sample_size}\")\n",
    "\n",
    "    sampled_dataframes = []\n",
    "    for file in files:\n",
    "        df = pd.read_parquet(file)\n",
    "        sampled_df = df.sample(n=sample_size, random_state=30, replace=False)\n",
    "        sampled_dataframes.append(sampled_df)\n",
    "    print(\"Finished sampling\")\n",
    "\n",
    "    if sampled_dataframes:\n",
    "        combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n",
    "        return combined_sampled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb0a333f-84b4-40c6-86e7-f2d56ecba837",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "taxi_data_dir = \"taxi_data\"\n",
    "download_parquet(taxi_urls, taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "882a02c9-5fcc-45ac-bdfc-c3b6fb6f1fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for all months: 384\n",
      "Finished sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NIU\\AppData\\Local\\Temp\\ipykernel_10180\\1458695063.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_sampled_df = pd.concat(sampled_dataframes, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "sampled_taxi_df = sample_monthly(taxi_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49122c36-026f-439e-97ff-82ea5515d31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>airport_fee</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-21 15:41:03</td>\n",
       "      <td>2020-01-21 15:54:32</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>233</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>14.14</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-29 21:06:16</td>\n",
       "      <td>2020-01-29 21:11:34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>141</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-26 20:19:48</td>\n",
       "      <td>2020-01-26 20:25:39</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-16 13:17:34</td>\n",
       "      <td>2020-01-16 13:24:01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>143</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.76</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-30 20:20:32</td>\n",
       "      <td>2020-01-30 20:28:30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>233</td>\n",
       "      <td>162</td>\n",
       "      <td>1</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-29 19:21:58</td>\n",
       "      <td>2020-01-29 19:35:52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>90</td>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.88</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-12 17:42:09</td>\n",
       "      <td>2020-01-12 17:47:26</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>236</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-29 22:03:48</td>\n",
       "      <td>2020-01-29 22:08:19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>142</td>\n",
       "      <td>239</td>\n",
       "      <td>2</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-01-21 21:17:23</td>\n",
       "      <td>2020-01-21 21:36:29</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>22.56</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-01-15 01:27:14</td>\n",
       "      <td>2020-01-15 01:32:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.70</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2020-01-21 15:41:03   2020-01-21 15:54:32              2.0   \n",
       "1         2  2020-01-29 21:06:16   2020-01-29 21:11:34              1.0   \n",
       "2         2  2020-01-26 20:19:48   2020-01-26 20:25:39              3.0   \n",
       "3         2  2020-01-16 13:17:34   2020-01-16 13:24:01              1.0   \n",
       "4         2  2020-01-30 20:20:32   2020-01-30 20:28:30              1.0   \n",
       "5         2  2020-01-29 19:21:58   2020-01-29 19:35:52              1.0   \n",
       "6         1  2020-01-12 17:42:09   2020-01-12 17:47:26              2.0   \n",
       "7         1  2020-01-29 22:03:48   2020-01-29 22:08:19              1.0   \n",
       "8         2  2020-01-21 21:17:23   2020-01-21 21:36:29              4.0   \n",
       "9         1  2020-01-15 01:27:14   2020-01-15 01:32:45              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           0.71         1.0                  N           233           164   \n",
       "1           0.76         1.0                  N           141           140   \n",
       "2           0.77         1.0                  N           161           163   \n",
       "3           0.99         1.0                  N           143           239   \n",
       "4           0.84         1.0                  N           233           162   \n",
       "5           1.54         1.0                  N            90           161   \n",
       "6           1.10         1.0                  N           140           236   \n",
       "7           0.90         1.0                  N           142           239   \n",
       "8           3.78         1.0                  N           161           148   \n",
       "9           0.90         1.0                  N           161           233   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             1          9.0    0.0      0.5        1.84           0.0   \n",
       "1             2          5.5    0.5      0.5        0.00           0.0   \n",
       "2             2          5.5    0.5      0.5        0.00           0.0   \n",
       "3             1          6.5    0.0      0.5        1.96           0.0   \n",
       "4             1          6.5    0.5      0.5        1.20           0.0   \n",
       "5             1         10.0    1.0      0.5        3.58           0.0   \n",
       "6             2          6.0    2.5      0.5        0.00           0.0   \n",
       "7             2          5.5    3.0      0.5        0.00           0.0   \n",
       "8             1         15.0    0.5      0.5        3.76           0.0   \n",
       "9             1          6.0    3.0      0.5        2.90           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \\\n",
       "0                    0.3         14.14                   2.5          NaN   \n",
       "1                    0.3          9.30                   2.5          NaN   \n",
       "2                    0.3          9.30                   2.5          NaN   \n",
       "3                    0.3         11.76                   2.5          NaN   \n",
       "4                    0.3         11.50                   2.5          NaN   \n",
       "5                    0.3         17.88                   2.5          NaN   \n",
       "6                    0.3          9.30                   2.5          NaN   \n",
       "7                    0.3          9.30                   2.5          NaN   \n",
       "8                    0.3         22.56                   2.5          NaN   \n",
       "9                    0.3         12.70                   2.5          NaN   \n",
       "\n",
       "   Airport_fee  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_taxi_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d712195d-1254-404a-8107-3eac3c796234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(dataframe):\n",
    "    try: \n",
    "        print(f\"Cleaning the sample dataframe...\")\n",
    "        \n",
    "\n",
    "        # look up the latitude and longitude (get those coordinates)\n",
    "        dataframe[[\"latitude_pickup\", \"longitude_pickup\"]] = dataframe[\"PULocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "        dataframe[[\"latitude_dropoff\", \"longitude_dropoff\"]] = dataframe[\"DOLocationID\"].map(ID_COORDS_DICT).apply(pd.Series)\n",
    "\n",
    "        \n",
    "        # remove some location IDs not valid and distance is 0\n",
    "        dataframe = dataframe.dropna(subset=['latitude_pickup', 'longitude_pickup', 'latitude_dropoff', 'longitude_dropoff'])\n",
    "        df = df[df[\"trip_distance\"] != 0]\n",
    "\n",
    "\n",
    "        # remove unnecessary columns\n",
    "        columns_to_keep = [\n",
    "            'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "            'trip_distance', \n",
    "            'latitude_pickup', 'longitude_pickup', 'latitude_dropoff', 'longitude_dropoff' , \n",
    "            'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee', \n",
    "            'total_amount'\n",
    "        ]         #调整\n",
    "        dataframe = dataframe[columns_to_keep]\n",
    "\n",
    "\n",
    "        # normalize column names\n",
    "        dataframe.columns = [col.lower().replace(' ', '_') for col in dataframe.columns]\n",
    "\n",
    "        # normalizing and using appropriate column types for the respective data;\n",
    "        dataframe['tpep_pickup_datetime'] = pd.to_datetime(dataframe['tpep_pickup_datetime'])\n",
    "        dataframe['tpep_dropoff_datetime'] = pd.to_datetime(dataframe['tpep_dropoff_datetime'])\n",
    "        dataframe['trip_distance'] = dataframe['trip_distance'].astype(float)\n",
    "\n",
    "        # for Yellow Taxi data, remove trips that start and/or end outside of  (40.560445, -74.242330) and (40.908524, -73.717047).\n",
    "        lat_min, lon_min = 40.560445, -74.242330\n",
    "        lat_max, lon_max = 40.908524, -73.717047\n",
    "        dataframe = dataframe[\n",
    "            (dataframe['latitude_pickup'].between(lat_min, lat_max)) &\n",
    "            (dataframe['longitude_pickup'].between(lon_min, lon_max)) &\n",
    "            (dataframe['latitude_dropoff'].between(lat_min, lat_max)) &\n",
    "            (dataframe['longitude_dropoff'].between(lon_min, lon_max))\n",
    "        ]\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing the dataframe: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ba7ad81-37ef-4bce-97cc-b107da25dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(directory):\n",
    "    all_dataframes = []\n",
    "    \n",
    "    if not isinstance(directory, str):\n",
    "        raise ValueError(\"Expected a directory path as a string.\")\n",
    "        \n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "    for parquet_url in files:\n",
    "        # to see if have downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        output_path = f\"{parquet_url.split('/')[-1].replace('.parquet', '')}.parquet\"  \n",
    "        if os.path.exists(output_path):\n",
    "            dataframe = pd.read_parquet(output_path) \n",
    "        else:\n",
    "            dataframe = pd.read_parquet(parquet_url)  \n",
    "        \n",
    "        cleaned_dataframe = get_and_clean_month(dataframe)\n",
    "        \n",
    "        if cleaned_dataframe is not None:\n",
    "            all_dataframes.append(cleaned_dataframe)\n",
    "\n",
    "    if all_dataframes:\n",
    "        return pd.concat(all_dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d136a7a-325f-465c-bb85-c5ee1218c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    all_urls = get_all_urls_from_taxi_page(TLC_URL)\n",
    "    yellow_urls, fhvhv_urls = all_urls\n",
    "    taxi_parquet = find_parquet_urls(yellow_urls)\n",
    "    taxi_data = get_and_clean_taxi_data(taxi_data_dir)  \n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the sample dataframe...\n"
     ]
    }
   ],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
